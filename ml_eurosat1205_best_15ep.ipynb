{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUKC08SRWbut"
      },
      "source": [
        "# **1: Download & Extract EuroSAT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUZXtuiQWbuu",
        "outputId": "3f63e63c-eaa6-4a5a-ef6b-d84ce43b5ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q rasterio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFxaS-icWbuv"
      },
      "source": [
        "# 2: Download and Unzip EuroSAT AllBands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dlYc2TwaWbuv"
      },
      "outputs": [],
      "source": [
        "!wget -q --no-check-certificate https://madm.dfki.de/files/sentinel/EuroSATallBands.zip -O EuroSATallBands.zip\n",
        "!unzip -q EuroSATallBands.zip -d EuroSAT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAgsGAnCWbuw"
      },
      "source": [
        "# 3: Convert .tif to .npy Using B11, B8, B4 + NDVI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4JNgJ5uWbuw",
        "outputId": "789a4ff7-f9e4-4c5b-f8c2-f4a6a79e282b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:08<00:00, 12.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " NPY files saved in EuroSAT_npy_AllBands_NDVI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from tqdm import tqdm\n",
        "\n",
        "def convert_selected_bands_with_ndvi(\n",
        "    src_root=\"EuroSAT/ds/images/remote_sensing/otherDatasets/sentinel_2/tif\",\n",
        "    dst_root=\"EuroSAT_npy_AllBands_NDVI\"\n",
        "):\n",
        "    os.makedirs(dst_root, exist_ok=True)\n",
        "    # All bands except B10 (index 9)\n",
        "    selected_indices = [1,2,3,4,5,6,7,8,10,11,12]  # B1-B9, B11-B12\n",
        "\n",
        "    for class_name in tqdm(os.listdir(src_root)):\n",
        "        src_class_path = os.path.join(src_root, class_name)\n",
        "        dst_class_path = os.path.join(dst_root, class_name)\n",
        "        os.makedirs(dst_class_path, exist_ok=True)\n",
        "\n",
        "        for fname in os.listdir(src_class_path):\n",
        "            if not fname.endswith(\".tif\"):\n",
        "                continue\n",
        "            path = os.path.join(src_class_path, fname)\n",
        "            out_path = os.path.join(dst_class_path, fname.replace(\".tif\", \".npy\"))\n",
        "\n",
        "            try:\n",
        "                with rasterio.open(path) as src:\n",
        "                    img = src.read()\n",
        "                    img = np.clip(img, 0, 10000).astype(np.float32)\n",
        "\n",
        "                    # Raw bands (scaled to 0‑1)\n",
        "                    raw_bands = img[selected_indices] / 10000.0\n",
        "\n",
        "                    # Indices\n",
        "                    B3, B4, B8, B11 = img[2], img[3], img[7], img[11]   # note: indices w.r.t selected set\n",
        "                    ndvi = (B8 - B4) / (B8 + B4 + 1e-6)\n",
        "                    ndwi = (B3 - B8) / (B3 + B8 + 1e-6)\n",
        "                    ndbi = (B11 - B8) / (B11 + B8 + 1e-6)\n",
        "                    ndmi = (B8 - B11) / (B8 + B11 + 1e-6)\n",
        "                    savi = (1.5 * (B8 - B4)) / (B8 + B4 + 0.5)\n",
        "\n",
        "                    indices = np.stack([ndvi, ndwi, ndbi, ndmi, savi], axis=0)\n",
        "                    indices = np.clip(indices, -1, 1)\n",
        "\n",
        "                    final = np.concatenate([raw_bands, indices], axis=0).astype(np.float32)\n",
        "                    np.save(out_path, final)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Failed: {fname} | {e}\")\n",
        "\n",
        "    print(f\" NPY files saved in {dst_root}\")\n",
        "\n",
        "convert_selected_bands_with_ndvi()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJjzLQvUWbux"
      },
      "source": [
        "# **4: Build Dataset Index + Compute Mean/Std**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ-w6SN8Wbuy",
        "outputId": "1fbae0d5-7842-4765-aeb1-5f7bf7653b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing mean/std: 100%|██████████| 27000/27000 [00:50<00:00, 531.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: [0.1117151603102684, 0.10418197512626648, 0.0946442037820816, 0.11991236358880997, 0.20029175281524658, 0.23738867044448853, 0.23010846972465515, 0.07321809232234955, 0.18205863237380981, 0.11181201785802841, 0.25996464490890503, 0.34858691692352295, -0.2825516164302826, -0.3988216817378998, 0.3988216817378998, 0.49681341648101807]\n",
            "Std: [0.03314068540930748, 0.03930474445223808, 0.05918382853269577, 0.05657161399722099, 0.08597585558891296, 0.10856421291828156, 0.11165442317724228, 0.04037296772003174, 0.10013400763273239, 0.07593318074941635, 0.12302295863628387, 0.3300042450428009, 0.3387872874736786, 0.27285003662109375, 0.27285003662109375, 0.4637760818004608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def build_dataset_index(npy_root=\"EuroSAT_npy_AllBands_NDVI\"):\n",
        "    rows = []\n",
        "    classes = sorted(os.listdir(npy_root))\n",
        "    for cls in classes:\n",
        "        for fname in os.listdir(os.path.join(npy_root, cls)):\n",
        "            if fname.endswith(\".npy\"):\n",
        "                rows.append({\n",
        "                    \"path\": os.path.join(npy_root, cls, fname),\n",
        "                    \"label\": cls\n",
        "                })\n",
        "    df = pd.DataFrame(rows)\n",
        "    df['label_idx'] = df['label'].astype('category').cat.codes\n",
        "    return df\n",
        "\n",
        "df = build_dataset_index()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, stratify=df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "def compute_mean_std(df):\n",
        "    channel_sum = 0\n",
        "    channel_squared_sum = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for path in tqdm(df['path'], desc=\"Computing mean/std\"):\n",
        "        arr = np.load(path)\n",
        "        c, h, w = arr.shape\n",
        "        channel_sum += arr.sum(axis=(1, 2))\n",
        "        channel_squared_sum += (arr ** 2).sum(axis=(1, 2))\n",
        "        total_pixels += h * w\n",
        "\n",
        "    mean = channel_sum / total_pixels\n",
        "    std = np.sqrt(channel_squared_sum / total_pixels - mean ** 2)\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "mean, std = compute_mean_std(df)\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Std:\", std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVQ2xSmNWbuy"
      },
      "source": [
        "# 5: Define Transforms****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gca18h-xWbuz"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9czxHkJoWbu0"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Already computed from your dataset\n",
        "#mean = [0.18205855758101852, 0.2301079463252315, 0.09464429615162037, 0.3485864438657407]\n",
        "#std = [0.10013492998282264, 0.11165543609217363, 0.059183771408410114, 0.33000484745181274]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(60),\n",
        "    transforms.Resize((64, 64)),\n",
        "    #transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    #transforms.Normalize(mean=mean, std=std)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaZ8OY2JWbu0"
      },
      "source": [
        "# 6: Dataset and DataLoader****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tztixX83Wbu1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "train_df, val_df = train_test_split(df, stratify=df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "class EuroSATDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        sample = np.load(row['path']).astype(np.float32)\n",
        "        sample = torch.tensor(sample)\n",
        "\n",
        "        # Calculate mean and std per sample\n",
        "        mean = sample.mean(dim=[1, 2], keepdim=True)\n",
        "        std = sample.std(dim=[1, 2], keepdim=True)\n",
        "\n",
        "        # Normalize the sample\n",
        "        sample = (sample - mean) / (std + 1e-5)  # Adding a small constant to avoid division by zero\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        label = torch.tensor(row['label_idx'], dtype=torch.long)\n",
        "        return sample, label\n",
        "\n",
        "train_ds = EuroSATDataset(train_df, transform=train_transform)\n",
        "val_ds = EuroSATDataset(val_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True,\n",
        "    num_workers=2, prefetch_factor=4, pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=64, shuffle=False,\n",
        "    num_workers=2, prefetch_factor=4, pin_memory=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjd-ixaUWbu1"
      },
      "source": [
        "**# 7: Define EfficientNet-B4 with 4 Channels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "owpgzAMdRDAI"
      },
      "outputs": [],
      "source": [
        "# ...existing code...\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ResNet50_MultiChannel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet50(weights=None)\n",
        "        # Modify first conv layer to accept 16 channels\n",
        "        self.model.conv1 = nn.Conv2d(16, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50_MultiChannel(num_classes=df['label_idx'].nunique())\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"✅ Using {torch.cuda.device_count()} GPUs via DataParallel\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "model = model.to(device)\n",
        "# ...existing code..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MFzORbuWbu2"
      },
      "source": [
        "**# 8: Loss + Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RsT9r718Wbu2"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSrPJ8-yWbu2"
      },
      "source": [
        "**# 9: Training + Validation Loops**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Rf_tEd3uWbu2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, epoch, log_interval=10):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    batch_loss = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(loader, 1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        batch_loss += loss.item()\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            avg_loss = batch_loss / log_interval\n",
        "            print(f\"[Epoch {epoch}] Batch {batch_idx:04d} - Avg Loss: {avg_loss:.4f}\")\n",
        "            batch_loss = 0\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device, class_names):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            correct += (out.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(out.argmax(1).cpu().numpy())\n",
        "\n",
        "    val_acc = correct / total\n",
        "    print(f\"\\n Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "    print(\" Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    return total_loss / total, val_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTNPhsgbWbu2"
      },
      "source": [
        "# **10: Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4qse7RL4Wbu2"
      },
      "outputs": [],
      "source": [
        "def run_training(train_loader, val_loader, df, num_epochs=10, save_path=\"./\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ResNet50_MultiChannel(num_classes=df['label_idx'].nunique())\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"✅ Using {torch.cuda.device_count()} GPUs via DataParallel\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5) # Added weight decay\n",
        "\n",
        "    best_acc = 0.0\n",
        "    class_names = sorted(df['label'].unique())\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device, class_names)\n",
        "\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f}, Acc: {train_acc * 100:.2f}\")\n",
        "        print(f\"Val Acc: {val_acc * 100:.2f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            model_path = f\"{save_path}/best_model_epoch_{epoch:02d}.pth\"\n",
        "            torch.save(\n",
        "    {\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"class_to_idx\": train_ds.df[[\"label\", \"label_idx\"]]\n",
        "                            .drop_duplicates()\n",
        "                            .set_index(\"label_idx\")[\"label\"]\n",
        "                            .to_dict()\n",
        "    },\n",
        "    model_path\n",
        ")\n",
        "            print(f\" Saved new best model: {model_path}\")\n",
        "\n",
        "    print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnoVPJXnWbu2"
      },
      "source": [
        "**Lights out and here we go**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HAcAGMLtWbu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bd407e-dd62-407f-ed80-d155d466cbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Batch 0010 - Avg Loss: 2.3583\n",
            "[Epoch 1] Batch 0020 - Avg Loss: 2.2767\n",
            "[Epoch 1] Batch 0030 - Avg Loss: 2.2861\n",
            "[Epoch 1] Batch 0040 - Avg Loss: 2.2709\n",
            "[Epoch 1] Batch 0050 - Avg Loss: 2.2841\n",
            "[Epoch 1] Batch 0060 - Avg Loss: 2.2353\n",
            "[Epoch 1] Batch 0070 - Avg Loss: 2.2420\n",
            "[Epoch 1] Batch 0080 - Avg Loss: 2.2061\n",
            "[Epoch 1] Batch 0090 - Avg Loss: 2.1816\n",
            "[Epoch 1] Batch 0100 - Avg Loss: 2.1750\n",
            "[Epoch 1] Batch 0110 - Avg Loss: 2.1469\n",
            "[Epoch 1] Batch 0120 - Avg Loss: 2.1903\n",
            "[Epoch 1] Batch 0130 - Avg Loss: 2.1153\n",
            "[Epoch 1] Batch 0140 - Avg Loss: 2.0819\n",
            "[Epoch 1] Batch 0150 - Avg Loss: 2.1036\n",
            "[Epoch 1] Batch 0160 - Avg Loss: 2.0504\n",
            "[Epoch 1] Batch 0170 - Avg Loss: 1.8860\n",
            "[Epoch 1] Batch 0180 - Avg Loss: 1.8858\n",
            "[Epoch 1] Batch 0190 - Avg Loss: 1.9002\n",
            "[Epoch 1] Batch 0200 - Avg Loss: 1.8339\n",
            "[Epoch 1] Batch 0210 - Avg Loss: 1.7457\n",
            "[Epoch 1] Batch 0220 - Avg Loss: 1.6414\n",
            "[Epoch 1] Batch 0230 - Avg Loss: 1.6655\n",
            "[Epoch 1] Batch 0240 - Avg Loss: 1.6598\n",
            "[Epoch 1] Batch 0250 - Avg Loss: 1.4721\n",
            "[Epoch 1] Batch 0260 - Avg Loss: 1.5090\n",
            "[Epoch 1] Batch 0270 - Avg Loss: 1.5084\n",
            "[Epoch 1] Batch 0280 - Avg Loss: 1.4527\n",
            "[Epoch 1] Batch 0290 - Avg Loss: 1.4339\n",
            "[Epoch 1] Batch 0300 - Avg Loss: 1.3479\n",
            "[Epoch 1] Batch 0310 - Avg Loss: 1.3127\n",
            "[Epoch 1] Batch 0320 - Avg Loss: 1.3530\n",
            "[Epoch 1] Batch 0330 - Avg Loss: 1.4407\n",
            "\n",
            " Val Acc: 52.13%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.74      0.73      0.73       600\n",
            "              Forest       0.38      0.70      0.49       600\n",
            "HerbaceousVegetation       0.37      0.39      0.38       600\n",
            "             Highway       0.34      0.19      0.25       500\n",
            "          Industrial       0.54      0.21      0.30       500\n",
            "             Pasture       0.42      0.25      0.31       400\n",
            "       PermanentCrop       0.34      0.19      0.24       500\n",
            "         Residential       0.53      0.75      0.63       600\n",
            "               River       0.71      0.89      0.79       500\n",
            "             SeaLake       0.71      0.73      0.72       600\n",
            "\n",
            "            accuracy                           0.52      5400\n",
            "           macro avg       0.51      0.50      0.48      5400\n",
            "        weighted avg       0.51      0.52      0.50      5400\n",
            "\n",
            "\n",
            "Train Loss: 1.8760, Acc: 32.41\n",
            "Val Acc: 52.13\n",
            " Saved new best model: .//best_model_epoch_01.pth\n",
            "[Epoch 2] Batch 0010 - Avg Loss: 1.3132\n",
            "[Epoch 2] Batch 0020 - Avg Loss: 1.2302\n",
            "[Epoch 2] Batch 0030 - Avg Loss: 1.2252\n",
            "[Epoch 2] Batch 0040 - Avg Loss: 1.2410\n",
            "[Epoch 2] Batch 0050 - Avg Loss: 1.1981\n",
            "[Epoch 2] Batch 0060 - Avg Loss: 1.2362\n",
            "[Epoch 2] Batch 0070 - Avg Loss: 1.3032\n",
            "[Epoch 2] Batch 0080 - Avg Loss: 1.1833\n",
            "[Epoch 2] Batch 0090 - Avg Loss: 1.1191\n",
            "[Epoch 2] Batch 0100 - Avg Loss: 1.0891\n",
            "[Epoch 2] Batch 0110 - Avg Loss: 1.1138\n",
            "[Epoch 2] Batch 0120 - Avg Loss: 1.0700\n",
            "[Epoch 2] Batch 0130 - Avg Loss: 1.1305\n",
            "[Epoch 2] Batch 0140 - Avg Loss: 1.1617\n",
            "[Epoch 2] Batch 0150 - Avg Loss: 1.1147\n",
            "[Epoch 2] Batch 0160 - Avg Loss: 1.1072\n",
            "[Epoch 2] Batch 0170 - Avg Loss: 1.0793\n",
            "[Epoch 2] Batch 0180 - Avg Loss: 0.9701\n",
            "[Epoch 2] Batch 0190 - Avg Loss: 1.0015\n",
            "[Epoch 2] Batch 0200 - Avg Loss: 1.0426\n",
            "[Epoch 2] Batch 0210 - Avg Loss: 0.9348\n",
            "[Epoch 2] Batch 0220 - Avg Loss: 0.9958\n",
            "[Epoch 2] Batch 0230 - Avg Loss: 0.9842\n",
            "[Epoch 2] Batch 0240 - Avg Loss: 0.9407\n",
            "[Epoch 2] Batch 0250 - Avg Loss: 0.9193\n",
            "[Epoch 2] Batch 0260 - Avg Loss: 0.9182\n",
            "[Epoch 2] Batch 0270 - Avg Loss: 0.8560\n",
            "[Epoch 2] Batch 0280 - Avg Loss: 0.8681\n",
            "[Epoch 2] Batch 0290 - Avg Loss: 0.8619\n",
            "[Epoch 2] Batch 0300 - Avg Loss: 0.8394\n",
            "[Epoch 2] Batch 0310 - Avg Loss: 0.8781\n",
            "[Epoch 2] Batch 0320 - Avg Loss: 0.7849\n",
            "[Epoch 2] Batch 0330 - Avg Loss: 0.9098\n",
            "\n",
            " Val Acc: 73.54%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.77      0.86      0.81       600\n",
            "              Forest       0.81      0.84      0.82       600\n",
            "HerbaceousVegetation       0.61      0.37      0.46       600\n",
            "             Highway       0.46      0.41      0.44       500\n",
            "          Industrial       0.79      0.73      0.76       500\n",
            "             Pasture       0.56      0.61      0.58       400\n",
            "       PermanentCrop       0.51      0.63      0.56       500\n",
            "         Residential       0.86      0.90      0.88       600\n",
            "               River       0.91      0.95      0.93       500\n",
            "             SeaLake       0.94      0.97      0.95       600\n",
            "\n",
            "            accuracy                           0.74      5400\n",
            "           macro avg       0.72      0.73      0.72      5400\n",
            "        weighted avg       0.73      0.74      0.73      5400\n",
            "\n",
            "\n",
            "Train Loss: 1.0431, Acc: 62.42\n",
            "Val Acc: 73.54\n",
            " Saved new best model: .//best_model_epoch_02.pth\n",
            "[Epoch 3] Batch 0010 - Avg Loss: 0.8331\n",
            "[Epoch 3] Batch 0020 - Avg Loss: 0.7311\n",
            "[Epoch 3] Batch 0030 - Avg Loss: 0.7327\n",
            "[Epoch 3] Batch 0040 - Avg Loss: 0.8068\n",
            "[Epoch 3] Batch 0050 - Avg Loss: 0.7085\n",
            "[Epoch 3] Batch 0060 - Avg Loss: 0.7290\n",
            "[Epoch 3] Batch 0070 - Avg Loss: 0.8154\n",
            "[Epoch 3] Batch 0080 - Avg Loss: 0.7513\n",
            "[Epoch 3] Batch 0090 - Avg Loss: 0.7087\n",
            "[Epoch 3] Batch 0100 - Avg Loss: 0.8379\n",
            "[Epoch 3] Batch 0110 - Avg Loss: 0.7517\n",
            "[Epoch 3] Batch 0120 - Avg Loss: 0.7907\n",
            "[Epoch 3] Batch 0130 - Avg Loss: 0.7041\n",
            "[Epoch 3] Batch 0140 - Avg Loss: 0.6516\n",
            "[Epoch 3] Batch 0150 - Avg Loss: 0.7017\n",
            "[Epoch 3] Batch 0160 - Avg Loss: 0.6854\n",
            "[Epoch 3] Batch 0170 - Avg Loss: 0.7096\n",
            "[Epoch 3] Batch 0180 - Avg Loss: 0.7500\n",
            "[Epoch 3] Batch 0190 - Avg Loss: 0.7646\n",
            "[Epoch 3] Batch 0200 - Avg Loss: 0.6990\n",
            "[Epoch 3] Batch 0210 - Avg Loss: 0.6784\n",
            "[Epoch 3] Batch 0220 - Avg Loss: 0.6608\n",
            "[Epoch 3] Batch 0230 - Avg Loss: 0.6207\n",
            "[Epoch 3] Batch 0240 - Avg Loss: 0.6423\n",
            "[Epoch 3] Batch 0250 - Avg Loss: 0.5905\n",
            "[Epoch 3] Batch 0260 - Avg Loss: 0.6235\n",
            "[Epoch 3] Batch 0270 - Avg Loss: 0.6566\n",
            "[Epoch 3] Batch 0280 - Avg Loss: 0.6563\n",
            "[Epoch 3] Batch 0290 - Avg Loss: 0.6720\n",
            "[Epoch 3] Batch 0300 - Avg Loss: 0.6462\n",
            "[Epoch 3] Batch 0310 - Avg Loss: 0.5905\n",
            "[Epoch 3] Batch 0320 - Avg Loss: 0.6513\n",
            "[Epoch 3] Batch 0330 - Avg Loss: 0.6161\n",
            "\n",
            " Val Acc: 80.65%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.86      0.88      0.87       600\n",
            "              Forest       0.85      0.92      0.88       600\n",
            "HerbaceousVegetation       0.61      0.79      0.69       600\n",
            "             Highway       0.74      0.50      0.59       500\n",
            "          Industrial       0.88      0.82      0.85       500\n",
            "             Pasture       0.76      0.55      0.64       400\n",
            "       PermanentCrop       0.62      0.56      0.59       500\n",
            "         Residential       0.83      0.97      0.90       600\n",
            "               River       0.94      0.92      0.93       500\n",
            "             SeaLake       0.97      0.99      0.98       600\n",
            "\n",
            "            accuracy                           0.81      5400\n",
            "           macro avg       0.81      0.79      0.79      5400\n",
            "        weighted avg       0.81      0.81      0.80      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.7015, Acc: 75.11\n",
            "Val Acc: 80.65\n",
            " Saved new best model: .//best_model_epoch_03.pth\n",
            "[Epoch 4] Batch 0010 - Avg Loss: 0.5983\n",
            "[Epoch 4] Batch 0020 - Avg Loss: 0.5448\n",
            "[Epoch 4] Batch 0030 - Avg Loss: 0.5742\n",
            "[Epoch 4] Batch 0040 - Avg Loss: 0.5901\n",
            "[Epoch 4] Batch 0050 - Avg Loss: 0.5886\n",
            "[Epoch 4] Batch 0060 - Avg Loss: 0.4968\n",
            "[Epoch 4] Batch 0070 - Avg Loss: 0.5601\n",
            "[Epoch 4] Batch 0080 - Avg Loss: 0.5661\n",
            "[Epoch 4] Batch 0090 - Avg Loss: 0.5582\n",
            "[Epoch 4] Batch 0100 - Avg Loss: 0.4692\n",
            "[Epoch 4] Batch 0110 - Avg Loss: 0.5577\n",
            "[Epoch 4] Batch 0120 - Avg Loss: 0.5330\n",
            "[Epoch 4] Batch 0130 - Avg Loss: 0.6279\n",
            "[Epoch 4] Batch 0140 - Avg Loss: 0.5357\n",
            "[Epoch 4] Batch 0150 - Avg Loss: 0.5355\n",
            "[Epoch 4] Batch 0160 - Avg Loss: 0.5536\n",
            "[Epoch 4] Batch 0170 - Avg Loss: 0.5277\n",
            "[Epoch 4] Batch 0180 - Avg Loss: 0.5129\n",
            "[Epoch 4] Batch 0190 - Avg Loss: 0.5445\n",
            "[Epoch 4] Batch 0200 - Avg Loss: 0.5261\n",
            "[Epoch 4] Batch 0210 - Avg Loss: 0.4999\n",
            "[Epoch 4] Batch 0220 - Avg Loss: 0.4908\n",
            "[Epoch 4] Batch 0230 - Avg Loss: 0.5014\n",
            "[Epoch 4] Batch 0240 - Avg Loss: 0.5035\n",
            "[Epoch 4] Batch 0250 - Avg Loss: 0.4975\n",
            "[Epoch 4] Batch 0260 - Avg Loss: 0.5572\n",
            "[Epoch 4] Batch 0270 - Avg Loss: 0.5065\n",
            "[Epoch 4] Batch 0280 - Avg Loss: 0.5491\n",
            "[Epoch 4] Batch 0290 - Avg Loss: 0.4825\n",
            "[Epoch 4] Batch 0300 - Avg Loss: 0.4556\n",
            "[Epoch 4] Batch 0310 - Avg Loss: 0.4845\n",
            "[Epoch 4] Batch 0320 - Avg Loss: 0.5511\n",
            "[Epoch 4] Batch 0330 - Avg Loss: 0.4987\n",
            "\n",
            " Val Acc: 83.59%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.72      0.95      0.82       600\n",
            "              Forest       0.96      0.93      0.94       600\n",
            "HerbaceousVegetation       0.77      0.67      0.71       600\n",
            "             Highway       0.72      0.65      0.68       500\n",
            "          Industrial       0.89      0.87      0.88       500\n",
            "             Pasture       0.89      0.64      0.74       400\n",
            "       PermanentCrop       0.60      0.72      0.66       500\n",
            "         Residential       0.98      0.91      0.94       600\n",
            "               River       0.94      0.96      0.95       500\n",
            "             SeaLake       0.96      0.97      0.97       600\n",
            "\n",
            "            accuracy                           0.84      5400\n",
            "           macro avg       0.84      0.83      0.83      5400\n",
            "        weighted avg       0.84      0.84      0.84      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.5322, Acc: 81.50\n",
            "Val Acc: 83.59\n",
            " Saved new best model: .//best_model_epoch_04.pth\n",
            "[Epoch 5] Batch 0010 - Avg Loss: 0.4806\n",
            "[Epoch 5] Batch 0020 - Avg Loss: 0.4878\n",
            "[Epoch 5] Batch 0030 - Avg Loss: 0.4678\n",
            "[Epoch 5] Batch 0040 - Avg Loss: 0.4503\n",
            "[Epoch 5] Batch 0050 - Avg Loss: 0.4208\n",
            "[Epoch 5] Batch 0060 - Avg Loss: 0.4343\n",
            "[Epoch 5] Batch 0070 - Avg Loss: 0.4787\n",
            "[Epoch 5] Batch 0080 - Avg Loss: 0.5706\n",
            "[Epoch 5] Batch 0090 - Avg Loss: 0.4842\n",
            "[Epoch 5] Batch 0100 - Avg Loss: 0.4390\n",
            "[Epoch 5] Batch 0110 - Avg Loss: 0.4554\n",
            "[Epoch 5] Batch 0120 - Avg Loss: 0.4691\n",
            "[Epoch 5] Batch 0130 - Avg Loss: 0.5004\n",
            "[Epoch 5] Batch 0140 - Avg Loss: 0.4735\n",
            "[Epoch 5] Batch 0150 - Avg Loss: 0.3976\n",
            "[Epoch 5] Batch 0160 - Avg Loss: 0.5175\n",
            "[Epoch 5] Batch 0170 - Avg Loss: 0.3813\n",
            "[Epoch 5] Batch 0180 - Avg Loss: 0.4060\n",
            "[Epoch 5] Batch 0190 - Avg Loss: 0.4522\n",
            "[Epoch 5] Batch 0200 - Avg Loss: 0.3918\n",
            "[Epoch 5] Batch 0210 - Avg Loss: 0.3874\n",
            "[Epoch 5] Batch 0220 - Avg Loss: 0.4444\n",
            "[Epoch 5] Batch 0230 - Avg Loss: 0.3900\n",
            "[Epoch 5] Batch 0240 - Avg Loss: 0.4735\n",
            "[Epoch 5] Batch 0250 - Avg Loss: 0.4343\n",
            "[Epoch 5] Batch 0260 - Avg Loss: 0.4062\n",
            "[Epoch 5] Batch 0270 - Avg Loss: 0.4839\n",
            "[Epoch 5] Batch 0280 - Avg Loss: 0.3704\n",
            "[Epoch 5] Batch 0290 - Avg Loss: 0.4300\n",
            "[Epoch 5] Batch 0300 - Avg Loss: 0.4625\n",
            "[Epoch 5] Batch 0310 - Avg Loss: 0.4023\n",
            "[Epoch 5] Batch 0320 - Avg Loss: 0.4251\n",
            "[Epoch 5] Batch 0330 - Avg Loss: 0.4194\n",
            "\n",
            " Val Acc: 86.54%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.86      0.91      0.88       600\n",
            "              Forest       0.96      0.94      0.95       600\n",
            "HerbaceousVegetation       0.84      0.68      0.75       600\n",
            "             Highway       0.67      0.85      0.75       500\n",
            "          Industrial       0.83      0.95      0.88       500\n",
            "             Pasture       0.80      0.82      0.81       400\n",
            "       PermanentCrop       0.76      0.65      0.70       500\n",
            "         Residential       0.97      0.88      0.92       600\n",
            "               River       0.96      0.96      0.96       500\n",
            "             SeaLake       0.98      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.87      5400\n",
            "           macro avg       0.86      0.86      0.86      5400\n",
            "        weighted avg       0.87      0.87      0.86      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.4436, Acc: 84.69\n",
            "Val Acc: 86.54\n",
            " Saved new best model: .//best_model_epoch_05.pth\n",
            "[Epoch 6] Batch 0010 - Avg Loss: 0.4100\n",
            "[Epoch 6] Batch 0020 - Avg Loss: 0.4255\n",
            "[Epoch 6] Batch 0030 - Avg Loss: 0.3920\n",
            "[Epoch 6] Batch 0040 - Avg Loss: 0.3748\n",
            "[Epoch 6] Batch 0050 - Avg Loss: 0.3698\n",
            "[Epoch 6] Batch 0060 - Avg Loss: 0.4019\n",
            "[Epoch 6] Batch 0070 - Avg Loss: 0.4043\n",
            "[Epoch 6] Batch 0080 - Avg Loss: 0.3794\n",
            "[Epoch 6] Batch 0090 - Avg Loss: 0.4117\n",
            "[Epoch 6] Batch 0100 - Avg Loss: 0.3896\n",
            "[Epoch 6] Batch 0110 - Avg Loss: 0.3513\n",
            "[Epoch 6] Batch 0120 - Avg Loss: 0.4133\n",
            "[Epoch 6] Batch 0130 - Avg Loss: 0.3904\n",
            "[Epoch 6] Batch 0140 - Avg Loss: 0.4140\n",
            "[Epoch 6] Batch 0150 - Avg Loss: 0.3556\n",
            "[Epoch 6] Batch 0160 - Avg Loss: 0.3621\n",
            "[Epoch 6] Batch 0170 - Avg Loss: 0.3516\n",
            "[Epoch 6] Batch 0180 - Avg Loss: 0.4213\n",
            "[Epoch 6] Batch 0190 - Avg Loss: 0.4833\n",
            "[Epoch 6] Batch 0200 - Avg Loss: 0.3634\n",
            "[Epoch 6] Batch 0210 - Avg Loss: 0.3375\n",
            "[Epoch 6] Batch 0220 - Avg Loss: 0.4164\n",
            "[Epoch 6] Batch 0230 - Avg Loss: 0.3932\n",
            "[Epoch 6] Batch 0240 - Avg Loss: 0.4030\n",
            "[Epoch 6] Batch 0250 - Avg Loss: 0.4799\n",
            "[Epoch 6] Batch 0260 - Avg Loss: 0.3565\n",
            "[Epoch 6] Batch 0270 - Avg Loss: 0.3198\n",
            "[Epoch 6] Batch 0280 - Avg Loss: 0.2826\n",
            "[Epoch 6] Batch 0290 - Avg Loss: 0.3296\n",
            "[Epoch 6] Batch 0300 - Avg Loss: 0.3894\n",
            "[Epoch 6] Batch 0310 - Avg Loss: 0.3643\n",
            "[Epoch 6] Batch 0320 - Avg Loss: 0.3446\n",
            "[Epoch 6] Batch 0330 - Avg Loss: 0.3874\n",
            "\n",
            " Val Acc: 89.74%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.94      0.87      0.90       600\n",
            "              Forest       0.95      0.98      0.96       600\n",
            "HerbaceousVegetation       0.77      0.86      0.81       600\n",
            "             Highway       0.88      0.79      0.83       500\n",
            "          Industrial       0.83      0.97      0.90       500\n",
            "             Pasture       0.85      0.85      0.85       400\n",
            "       PermanentCrop       0.84      0.69      0.76       500\n",
            "         Residential       0.97      0.94      0.95       600\n",
            "               River       0.95      0.98      0.97       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.90      5400\n",
            "           macro avg       0.90      0.89      0.89      5400\n",
            "        weighted avg       0.90      0.90      0.90      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.3828, Acc: 86.81\n",
            "Val Acc: 89.74\n",
            " Saved new best model: .//best_model_epoch_06.pth\n",
            "[Epoch 7] Batch 0010 - Avg Loss: 0.3294\n",
            "[Epoch 7] Batch 0020 - Avg Loss: 0.3404\n",
            "[Epoch 7] Batch 0030 - Avg Loss: 0.3302\n",
            "[Epoch 7] Batch 0040 - Avg Loss: 0.3412\n",
            "[Epoch 7] Batch 0050 - Avg Loss: 0.3452\n",
            "[Epoch 7] Batch 0060 - Avg Loss: 0.3264\n",
            "[Epoch 7] Batch 0070 - Avg Loss: 0.3428\n",
            "[Epoch 7] Batch 0080 - Avg Loss: 0.3596\n",
            "[Epoch 7] Batch 0090 - Avg Loss: 0.3568\n",
            "[Epoch 7] Batch 0100 - Avg Loss: 0.3368\n",
            "[Epoch 7] Batch 0110 - Avg Loss: 0.4048\n",
            "[Epoch 7] Batch 0120 - Avg Loss: 0.3305\n",
            "[Epoch 7] Batch 0130 - Avg Loss: 0.4065\n",
            "[Epoch 7] Batch 0140 - Avg Loss: 0.3147\n",
            "[Epoch 7] Batch 0150 - Avg Loss: 0.3041\n",
            "[Epoch 7] Batch 0160 - Avg Loss: 0.3968\n",
            "[Epoch 7] Batch 0170 - Avg Loss: 0.3437\n",
            "[Epoch 7] Batch 0180 - Avg Loss: 0.4146\n",
            "[Epoch 7] Batch 0190 - Avg Loss: 0.3525\n",
            "[Epoch 7] Batch 0200 - Avg Loss: 0.3867\n",
            "[Epoch 7] Batch 0210 - Avg Loss: 0.3772\n",
            "[Epoch 7] Batch 0220 - Avg Loss: 0.3454\n",
            "[Epoch 7] Batch 0230 - Avg Loss: 0.3065\n",
            "[Epoch 7] Batch 0240 - Avg Loss: 0.3797\n",
            "[Epoch 7] Batch 0250 - Avg Loss: 0.2573\n",
            "[Epoch 7] Batch 0260 - Avg Loss: 0.2705\n",
            "[Epoch 7] Batch 0270 - Avg Loss: 0.3392\n",
            "[Epoch 7] Batch 0280 - Avg Loss: 0.3433\n",
            "[Epoch 7] Batch 0290 - Avg Loss: 0.3294\n",
            "[Epoch 7] Batch 0300 - Avg Loss: 0.3282\n",
            "[Epoch 7] Batch 0310 - Avg Loss: 0.3575\n",
            "[Epoch 7] Batch 0320 - Avg Loss: 0.3511\n",
            "[Epoch 7] Batch 0330 - Avg Loss: 0.3081\n",
            "\n",
            " Val Acc: 90.30%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.86      0.95      0.90       600\n",
            "              Forest       0.99      0.94      0.96       600\n",
            "HerbaceousVegetation       0.83      0.80      0.81       600\n",
            "             Highway       0.86      0.84      0.85       500\n",
            "          Industrial       0.92      0.95      0.93       500\n",
            "             Pasture       0.90      0.81      0.85       400\n",
            "       PermanentCrop       0.77      0.81      0.79       500\n",
            "         Residential       0.96      0.95      0.96       600\n",
            "               River       0.96      0.95      0.96       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.90      5400\n",
            "           macro avg       0.90      0.90      0.90      5400\n",
            "        weighted avg       0.90      0.90      0.90      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.3435, Acc: 87.99\n",
            "Val Acc: 90.30\n",
            " Saved new best model: .//best_model_epoch_07.pth\n",
            "[Epoch 8] Batch 0010 - Avg Loss: 0.2751\n",
            "[Epoch 8] Batch 0020 - Avg Loss: 0.3427\n",
            "[Epoch 8] Batch 0030 - Avg Loss: 0.2682\n",
            "[Epoch 8] Batch 0040 - Avg Loss: 0.3864\n",
            "[Epoch 8] Batch 0050 - Avg Loss: 0.2906\n",
            "[Epoch 8] Batch 0060 - Avg Loss: 0.3361\n",
            "[Epoch 8] Batch 0070 - Avg Loss: 0.3353\n",
            "[Epoch 8] Batch 0080 - Avg Loss: 0.2938\n",
            "[Epoch 8] Batch 0090 - Avg Loss: 0.3310\n",
            "[Epoch 8] Batch 0100 - Avg Loss: 0.3337\n",
            "[Epoch 8] Batch 0110 - Avg Loss: 0.3434\n",
            "[Epoch 8] Batch 0120 - Avg Loss: 0.3027\n",
            "[Epoch 8] Batch 0130 - Avg Loss: 0.3039\n",
            "[Epoch 8] Batch 0140 - Avg Loss: 0.2921\n",
            "[Epoch 8] Batch 0150 - Avg Loss: 0.3317\n",
            "[Epoch 8] Batch 0160 - Avg Loss: 0.2846\n",
            "[Epoch 8] Batch 0170 - Avg Loss: 0.3164\n",
            "[Epoch 8] Batch 0180 - Avg Loss: 0.3054\n",
            "[Epoch 8] Batch 0190 - Avg Loss: 0.2942\n",
            "[Epoch 8] Batch 0200 - Avg Loss: 0.2972\n",
            "[Epoch 8] Batch 0210 - Avg Loss: 0.2289\n",
            "[Epoch 8] Batch 0220 - Avg Loss: 0.2712\n",
            "[Epoch 8] Batch 0230 - Avg Loss: 0.2918\n",
            "[Epoch 8] Batch 0240 - Avg Loss: 0.2574\n",
            "[Epoch 8] Batch 0250 - Avg Loss: 0.2990\n",
            "[Epoch 8] Batch 0260 - Avg Loss: 0.2993\n",
            "[Epoch 8] Batch 0270 - Avg Loss: 0.3408\n",
            "[Epoch 8] Batch 0280 - Avg Loss: 0.2762\n",
            "[Epoch 8] Batch 0290 - Avg Loss: 0.3278\n",
            "[Epoch 8] Batch 0300 - Avg Loss: 0.2711\n",
            "[Epoch 8] Batch 0310 - Avg Loss: 0.3381\n",
            "[Epoch 8] Batch 0320 - Avg Loss: 0.2861\n",
            "[Epoch 8] Batch 0330 - Avg Loss: 0.3020\n",
            "\n",
            " Val Acc: 91.63%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.95      0.89      0.92       600\n",
            "              Forest       0.94      0.99      0.96       600\n",
            "HerbaceousVegetation       0.82      0.84      0.83       600\n",
            "             Highway       0.91      0.86      0.89       500\n",
            "          Industrial       0.92      0.93      0.92       500\n",
            "             Pasture       0.86      0.90      0.88       400\n",
            "       PermanentCrop       0.83      0.82      0.83       500\n",
            "         Residential       0.97      0.96      0.97       600\n",
            "               River       0.96      0.97      0.97       500\n",
            "             SeaLake       0.99      0.98      0.99       600\n",
            "\n",
            "            accuracy                           0.92      5400\n",
            "           macro avg       0.91      0.91      0.91      5400\n",
            "        weighted avg       0.92      0.92      0.92      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.3051, Acc: 89.47\n",
            "Val Acc: 91.63\n",
            " Saved new best model: .//best_model_epoch_08.pth\n",
            "[Epoch 9] Batch 0010 - Avg Loss: 0.2875\n",
            "[Epoch 9] Batch 0020 - Avg Loss: 0.2801\n",
            "[Epoch 9] Batch 0030 - Avg Loss: 0.3029\n",
            "[Epoch 9] Batch 0040 - Avg Loss: 0.2734\n",
            "[Epoch 9] Batch 0050 - Avg Loss: 0.2823\n",
            "[Epoch 9] Batch 0060 - Avg Loss: 0.3380\n",
            "[Epoch 9] Batch 0070 - Avg Loss: 0.2982\n",
            "[Epoch 9] Batch 0080 - Avg Loss: 0.1977\n",
            "[Epoch 9] Batch 0090 - Avg Loss: 0.2503\n",
            "[Epoch 9] Batch 0100 - Avg Loss: 0.2952\n",
            "[Epoch 9] Batch 0110 - Avg Loss: 0.3364\n",
            "[Epoch 9] Batch 0120 - Avg Loss: 0.3173\n",
            "[Epoch 9] Batch 0130 - Avg Loss: 0.2665\n",
            "[Epoch 9] Batch 0140 - Avg Loss: 0.3491\n",
            "[Epoch 9] Batch 0150 - Avg Loss: 0.2583\n",
            "[Epoch 9] Batch 0160 - Avg Loss: 0.2669\n",
            "[Epoch 9] Batch 0170 - Avg Loss: 0.2824\n",
            "[Epoch 9] Batch 0180 - Avg Loss: 0.2729\n",
            "[Epoch 9] Batch 0190 - Avg Loss: 0.3212\n",
            "[Epoch 9] Batch 0200 - Avg Loss: 0.2273\n",
            "[Epoch 9] Batch 0210 - Avg Loss: 0.2648\n",
            "[Epoch 9] Batch 0220 - Avg Loss: 0.3179\n",
            "[Epoch 9] Batch 0230 - Avg Loss: 0.2754\n",
            "[Epoch 9] Batch 0240 - Avg Loss: 0.3506\n",
            "[Epoch 9] Batch 0250 - Avg Loss: 0.2763\n",
            "[Epoch 9] Batch 0260 - Avg Loss: 0.3481\n",
            "[Epoch 9] Batch 0270 - Avg Loss: 0.2446\n",
            "[Epoch 9] Batch 0280 - Avg Loss: 0.2630\n",
            "[Epoch 9] Batch 0290 - Avg Loss: 0.2292\n",
            "[Epoch 9] Batch 0300 - Avg Loss: 0.2075\n",
            "[Epoch 9] Batch 0310 - Avg Loss: 0.2416\n",
            "[Epoch 9] Batch 0320 - Avg Loss: 0.2670\n",
            "[Epoch 9] Batch 0330 - Avg Loss: 0.2651\n",
            "\n",
            " Val Acc: 91.22%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.96      0.89      0.92       600\n",
            "              Forest       0.99      0.95      0.97       600\n",
            "HerbaceousVegetation       0.86      0.77      0.81       600\n",
            "             Highway       0.91      0.85      0.88       500\n",
            "          Industrial       0.91      0.96      0.93       500\n",
            "             Pasture       0.91      0.84      0.87       400\n",
            "       PermanentCrop       0.73      0.90      0.81       500\n",
            "         Residential       0.93      0.98      0.95       600\n",
            "               River       0.94      0.98      0.96       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.91      5400\n",
            "           macro avg       0.91      0.91      0.91      5400\n",
            "        weighted avg       0.92      0.91      0.91      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.2805, Acc: 90.58\n",
            "Val Acc: 91.22\n",
            "[Epoch 10] Batch 0010 - Avg Loss: 0.2999\n",
            "[Epoch 10] Batch 0020 - Avg Loss: 0.2262\n",
            "[Epoch 10] Batch 0030 - Avg Loss: 0.2610\n",
            "[Epoch 10] Batch 0040 - Avg Loss: 0.2375\n",
            "[Epoch 10] Batch 0050 - Avg Loss: 0.2464\n",
            "[Epoch 10] Batch 0060 - Avg Loss: 0.2430\n",
            "[Epoch 10] Batch 0070 - Avg Loss: 0.2139\n",
            "[Epoch 10] Batch 0080 - Avg Loss: 0.2632\n",
            "[Epoch 10] Batch 0090 - Avg Loss: 0.2352\n",
            "[Epoch 10] Batch 0100 - Avg Loss: 0.2489\n",
            "[Epoch 10] Batch 0110 - Avg Loss: 0.2452\n",
            "[Epoch 10] Batch 0120 - Avg Loss: 0.2741\n",
            "[Epoch 10] Batch 0130 - Avg Loss: 0.3094\n",
            "[Epoch 10] Batch 0140 - Avg Loss: 0.1911\n",
            "[Epoch 10] Batch 0150 - Avg Loss: 0.2480\n",
            "[Epoch 10] Batch 0160 - Avg Loss: 0.2946\n",
            "[Epoch 10] Batch 0170 - Avg Loss: 0.2319\n",
            "[Epoch 10] Batch 0180 - Avg Loss: 0.3101\n",
            "[Epoch 10] Batch 0190 - Avg Loss: 0.2177\n",
            "[Epoch 10] Batch 0200 - Avg Loss: 0.2463\n",
            "[Epoch 10] Batch 0210 - Avg Loss: 0.3227\n",
            "[Epoch 10] Batch 0220 - Avg Loss: 0.2283\n",
            "[Epoch 10] Batch 0230 - Avg Loss: 0.2662\n",
            "[Epoch 10] Batch 0240 - Avg Loss: 0.2815\n",
            "[Epoch 10] Batch 0250 - Avg Loss: 0.2777\n",
            "[Epoch 10] Batch 0260 - Avg Loss: 0.2331\n",
            "[Epoch 10] Batch 0270 - Avg Loss: 0.2901\n",
            "[Epoch 10] Batch 0280 - Avg Loss: 0.2980\n",
            "[Epoch 10] Batch 0290 - Avg Loss: 0.2829\n",
            "[Epoch 10] Batch 0300 - Avg Loss: 0.2558\n",
            "[Epoch 10] Batch 0310 - Avg Loss: 0.2446\n",
            "[Epoch 10] Batch 0320 - Avg Loss: 0.2071\n",
            "[Epoch 10] Batch 0330 - Avg Loss: 0.2495\n",
            "\n",
            " Val Acc: 93.00%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.95      0.92      0.93       600\n",
            "              Forest       0.98      0.98      0.98       600\n",
            "HerbaceousVegetation       0.83      0.88      0.85       600\n",
            "             Highway       0.93      0.86      0.90       500\n",
            "          Industrial       0.94      0.95      0.94       500\n",
            "             Pasture       0.93      0.90      0.92       400\n",
            "       PermanentCrop       0.86      0.84      0.85       500\n",
            "         Residential       0.96      0.97      0.96       600\n",
            "               River       0.95      0.98      0.96       500\n",
            "             SeaLake       0.98      1.00      0.99       600\n",
            "\n",
            "            accuracy                           0.93      5400\n",
            "           macro avg       0.93      0.93      0.93      5400\n",
            "        weighted avg       0.93      0.93      0.93      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.2591, Acc: 91.28\n",
            "Val Acc: 93.00\n",
            " Saved new best model: .//best_model_epoch_10.pth\n",
            "[Epoch 11] Batch 0010 - Avg Loss: 0.2286\n",
            "[Epoch 11] Batch 0020 - Avg Loss: 0.2819\n",
            "[Epoch 11] Batch 0030 - Avg Loss: 0.1928\n",
            "[Epoch 11] Batch 0040 - Avg Loss: 0.2229\n",
            "[Epoch 11] Batch 0050 - Avg Loss: 0.2465\n",
            "[Epoch 11] Batch 0060 - Avg Loss: 0.1868\n",
            "[Epoch 11] Batch 0070 - Avg Loss: 0.2320\n",
            "[Epoch 11] Batch 0080 - Avg Loss: 0.2290\n",
            "[Epoch 11] Batch 0090 - Avg Loss: 0.2066\n",
            "[Epoch 11] Batch 0100 - Avg Loss: 0.2233\n",
            "[Epoch 11] Batch 0110 - Avg Loss: 0.2870\n",
            "[Epoch 11] Batch 0120 - Avg Loss: 0.2719\n",
            "[Epoch 11] Batch 0130 - Avg Loss: 0.2259\n",
            "[Epoch 11] Batch 0140 - Avg Loss: 0.2386\n",
            "[Epoch 11] Batch 0150 - Avg Loss: 0.2443\n",
            "[Epoch 11] Batch 0160 - Avg Loss: 0.2498\n",
            "[Epoch 11] Batch 0170 - Avg Loss: 0.2341\n",
            "[Epoch 11] Batch 0180 - Avg Loss: 0.2683\n",
            "[Epoch 11] Batch 0190 - Avg Loss: 0.3040\n",
            "[Epoch 11] Batch 0200 - Avg Loss: 0.2207\n",
            "[Epoch 11] Batch 0210 - Avg Loss: 0.2275\n",
            "[Epoch 11] Batch 0220 - Avg Loss: 0.2666\n",
            "[Epoch 11] Batch 0230 - Avg Loss: 0.1850\n",
            "[Epoch 11] Batch 0240 - Avg Loss: 0.2299\n",
            "[Epoch 11] Batch 0250 - Avg Loss: 0.2499\n",
            "[Epoch 11] Batch 0260 - Avg Loss: 0.2927\n",
            "[Epoch 11] Batch 0270 - Avg Loss: 0.3165\n",
            "[Epoch 11] Batch 0280 - Avg Loss: 0.2285\n",
            "[Epoch 11] Batch 0290 - Avg Loss: 0.2368\n",
            "[Epoch 11] Batch 0300 - Avg Loss: 0.2278\n",
            "[Epoch 11] Batch 0310 - Avg Loss: 0.1918\n",
            "[Epoch 11] Batch 0320 - Avg Loss: 0.2795\n",
            "[Epoch 11] Batch 0330 - Avg Loss: 0.2098\n",
            "\n",
            " Val Acc: 92.04%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.90      0.93      0.92       600\n",
            "              Forest       0.97      0.98      0.97       600\n",
            "HerbaceousVegetation       0.83      0.87      0.85       600\n",
            "             Highway       0.84      0.91      0.88       500\n",
            "          Industrial       0.95      0.95      0.95       500\n",
            "             Pasture       0.92      0.89      0.90       400\n",
            "       PermanentCrop       0.91      0.68      0.78       500\n",
            "         Residential       0.95      0.98      0.97       600\n",
            "               River       0.94      0.98      0.96       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.92      5400\n",
            "           macro avg       0.92      0.92      0.92      5400\n",
            "        weighted avg       0.92      0.92      0.92      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.2401, Acc: 92.00\n",
            "Val Acc: 92.04\n",
            "[Epoch 12] Batch 0010 - Avg Loss: 0.2881\n",
            "[Epoch 12] Batch 0020 - Avg Loss: 0.2308\n",
            "[Epoch 12] Batch 0030 - Avg Loss: 0.2330\n",
            "[Epoch 12] Batch 0040 - Avg Loss: 0.1819\n",
            "[Epoch 12] Batch 0050 - Avg Loss: 0.2335\n",
            "[Epoch 12] Batch 0060 - Avg Loss: 0.2673\n",
            "[Epoch 12] Batch 0070 - Avg Loss: 0.2078\n",
            "[Epoch 12] Batch 0080 - Avg Loss: 0.2353\n",
            "[Epoch 12] Batch 0090 - Avg Loss: 0.3004\n",
            "[Epoch 12] Batch 0100 - Avg Loss: 0.2087\n",
            "[Epoch 12] Batch 0110 - Avg Loss: 0.2703\n",
            "[Epoch 12] Batch 0120 - Avg Loss: 0.2632\n",
            "[Epoch 12] Batch 0130 - Avg Loss: 0.2013\n",
            "[Epoch 12] Batch 0140 - Avg Loss: 0.1874\n",
            "[Epoch 12] Batch 0150 - Avg Loss: 0.2116\n",
            "[Epoch 12] Batch 0160 - Avg Loss: 0.2467\n",
            "[Epoch 12] Batch 0170 - Avg Loss: 0.2193\n",
            "[Epoch 12] Batch 0180 - Avg Loss: 0.2220\n",
            "[Epoch 12] Batch 0190 - Avg Loss: 0.1816\n",
            "[Epoch 12] Batch 0200 - Avg Loss: 0.2837\n",
            "[Epoch 12] Batch 0210 - Avg Loss: 0.2462\n",
            "[Epoch 12] Batch 0220 - Avg Loss: 0.2476\n",
            "[Epoch 12] Batch 0230 - Avg Loss: 0.2063\n",
            "[Epoch 12] Batch 0240 - Avg Loss: 0.2496\n",
            "[Epoch 12] Batch 0250 - Avg Loss: 0.2256\n",
            "[Epoch 12] Batch 0260 - Avg Loss: 0.2243\n",
            "[Epoch 12] Batch 0270 - Avg Loss: 0.2181\n",
            "[Epoch 12] Batch 0280 - Avg Loss: 0.2731\n",
            "[Epoch 12] Batch 0290 - Avg Loss: 0.2143\n",
            "[Epoch 12] Batch 0300 - Avg Loss: 0.2178\n",
            "[Epoch 12] Batch 0310 - Avg Loss: 0.2448\n",
            "[Epoch 12] Batch 0320 - Avg Loss: 0.2458\n",
            "[Epoch 12] Batch 0330 - Avg Loss: 0.2138\n",
            "\n",
            " Val Acc: 92.91%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.89      0.94      0.92       600\n",
            "              Forest       0.99      0.97      0.98       600\n",
            "HerbaceousVegetation       0.90      0.82      0.86       600\n",
            "             Highway       0.96      0.82      0.88       500\n",
            "          Industrial       0.91      0.97      0.94       500\n",
            "             Pasture       0.92      0.92      0.92       400\n",
            "       PermanentCrop       0.84      0.88      0.86       500\n",
            "         Residential       0.98      0.97      0.97       600\n",
            "               River       0.91      0.99      0.95       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.93      5400\n",
            "           macro avg       0.93      0.93      0.93      5400\n",
            "        weighted avg       0.93      0.93      0.93      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.2322, Acc: 92.15\n",
            "Val Acc: 92.91\n",
            "[Epoch 13] Batch 0010 - Avg Loss: 0.2219\n",
            "[Epoch 13] Batch 0020 - Avg Loss: 0.2440\n",
            "[Epoch 13] Batch 0030 - Avg Loss: 0.1676\n",
            "[Epoch 13] Batch 0040 - Avg Loss: 0.2113\n",
            "[Epoch 13] Batch 0050 - Avg Loss: 0.1853\n",
            "[Epoch 13] Batch 0060 - Avg Loss: 0.2320\n",
            "[Epoch 13] Batch 0070 - Avg Loss: 0.2268\n",
            "[Epoch 13] Batch 0080 - Avg Loss: 0.1768\n",
            "[Epoch 13] Batch 0090 - Avg Loss: 0.2173\n",
            "[Epoch 13] Batch 0100 - Avg Loss: 0.1799\n",
            "[Epoch 13] Batch 0110 - Avg Loss: 0.1865\n",
            "[Epoch 13] Batch 0120 - Avg Loss: 0.2122\n",
            "[Epoch 13] Batch 0130 - Avg Loss: 0.2000\n",
            "[Epoch 13] Batch 0140 - Avg Loss: 0.2195\n",
            "[Epoch 13] Batch 0150 - Avg Loss: 0.1702\n",
            "[Epoch 13] Batch 0160 - Avg Loss: 0.1718\n",
            "[Epoch 13] Batch 0170 - Avg Loss: 0.2262\n",
            "[Epoch 13] Batch 0180 - Avg Loss: 0.1862\n",
            "[Epoch 13] Batch 0190 - Avg Loss: 0.1970\n",
            "[Epoch 13] Batch 0200 - Avg Loss: 0.2174\n",
            "[Epoch 13] Batch 0210 - Avg Loss: 0.2565\n",
            "[Epoch 13] Batch 0220 - Avg Loss: 0.2297\n",
            "[Epoch 13] Batch 0230 - Avg Loss: 0.2113\n",
            "[Epoch 13] Batch 0240 - Avg Loss: 0.2006\n",
            "[Epoch 13] Batch 0250 - Avg Loss: 0.1991\n",
            "[Epoch 13] Batch 0260 - Avg Loss: 0.2603\n",
            "[Epoch 13] Batch 0270 - Avg Loss: 0.2259\n",
            "[Epoch 13] Batch 0280 - Avg Loss: 0.2234\n",
            "[Epoch 13] Batch 0290 - Avg Loss: 0.2277\n",
            "[Epoch 13] Batch 0300 - Avg Loss: 0.2333\n",
            "[Epoch 13] Batch 0310 - Avg Loss: 0.1771\n",
            "[Epoch 13] Batch 0320 - Avg Loss: 0.2443\n",
            "[Epoch 13] Batch 0330 - Avg Loss: 0.2561\n",
            "\n",
            " Val Acc: 93.17%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.97      0.90      0.93       600\n",
            "              Forest       0.98      0.98      0.98       600\n",
            "HerbaceousVegetation       0.90      0.79      0.85       600\n",
            "             Highway       0.95      0.91      0.93       500\n",
            "          Industrial       0.93      0.98      0.96       500\n",
            "             Pasture       0.91      0.89      0.90       400\n",
            "       PermanentCrop       0.77      0.92      0.84       500\n",
            "         Residential       0.98      0.96      0.97       600\n",
            "               River       0.95      0.99      0.97       500\n",
            "             SeaLake       0.98      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.93      5400\n",
            "           macro avg       0.93      0.93      0.93      5400\n",
            "        weighted avg       0.93      0.93      0.93      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.2124, Acc: 92.55\n",
            "Val Acc: 93.17\n",
            " Saved new best model: .//best_model_epoch_13.pth\n",
            "[Epoch 14] Batch 0010 - Avg Loss: 0.1776\n",
            "[Epoch 14] Batch 0020 - Avg Loss: 0.1800\n",
            "[Epoch 14] Batch 0030 - Avg Loss: 0.1971\n",
            "[Epoch 14] Batch 0040 - Avg Loss: 0.2202\n",
            "[Epoch 14] Batch 0050 - Avg Loss: 0.2090\n",
            "[Epoch 14] Batch 0060 - Avg Loss: 0.2357\n",
            "[Epoch 14] Batch 0070 - Avg Loss: 0.1928\n",
            "[Epoch 14] Batch 0080 - Avg Loss: 0.2524\n",
            "[Epoch 14] Batch 0090 - Avg Loss: 0.1883\n",
            "[Epoch 14] Batch 0100 - Avg Loss: 0.1873\n",
            "[Epoch 14] Batch 0110 - Avg Loss: 0.1419\n",
            "[Epoch 14] Batch 0120 - Avg Loss: 0.2242\n",
            "[Epoch 14] Batch 0130 - Avg Loss: 0.1956\n",
            "[Epoch 14] Batch 0140 - Avg Loss: 0.1988\n",
            "[Epoch 14] Batch 0150 - Avg Loss: 0.1683\n",
            "[Epoch 14] Batch 0160 - Avg Loss: 0.1646\n",
            "[Epoch 14] Batch 0170 - Avg Loss: 0.1833\n",
            "[Epoch 14] Batch 0180 - Avg Loss: 0.2146\n",
            "[Epoch 14] Batch 0190 - Avg Loss: 0.2443\n",
            "[Epoch 14] Batch 0200 - Avg Loss: 0.1659\n",
            "[Epoch 14] Batch 0210 - Avg Loss: 0.1833\n",
            "[Epoch 14] Batch 0220 - Avg Loss: 0.2036\n",
            "[Epoch 14] Batch 0230 - Avg Loss: 0.2023\n",
            "[Epoch 14] Batch 0240 - Avg Loss: 0.2195\n",
            "[Epoch 14] Batch 0250 - Avg Loss: 0.1881\n",
            "[Epoch 14] Batch 0260 - Avg Loss: 0.1826\n",
            "[Epoch 14] Batch 0270 - Avg Loss: 0.2119\n",
            "[Epoch 14] Batch 0280 - Avg Loss: 0.2115\n",
            "[Epoch 14] Batch 0290 - Avg Loss: 0.2045\n",
            "[Epoch 14] Batch 0300 - Avg Loss: 0.1940\n",
            "[Epoch 14] Batch 0310 - Avg Loss: 0.1683\n",
            "[Epoch 14] Batch 0320 - Avg Loss: 0.2421\n",
            "[Epoch 14] Batch 0330 - Avg Loss: 0.1743\n",
            "\n",
            " Val Acc: 92.80%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.95      0.91      0.93       600\n",
            "              Forest       0.97      0.97      0.97       600\n",
            "HerbaceousVegetation       0.85      0.90      0.87       600\n",
            "             Highway       0.80      0.97      0.88       500\n",
            "          Industrial       0.98      0.88      0.93       500\n",
            "             Pasture       0.93      0.86      0.89       400\n",
            "       PermanentCrop       0.87      0.82      0.85       500\n",
            "         Residential       0.98      0.97      0.98       600\n",
            "               River       0.97      0.98      0.97       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.93      5400\n",
            "           macro avg       0.93      0.92      0.93      5400\n",
            "        weighted avg       0.93      0.93      0.93      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1972, Acc: 93.19\n",
            "Val Acc: 92.80\n",
            "[Epoch 15] Batch 0010 - Avg Loss: 0.2360\n",
            "[Epoch 15] Batch 0020 - Avg Loss: 0.1452\n",
            "[Epoch 15] Batch 0030 - Avg Loss: 0.1963\n",
            "[Epoch 15] Batch 0040 - Avg Loss: 0.1671\n",
            "[Epoch 15] Batch 0050 - Avg Loss: 0.2314\n",
            "[Epoch 15] Batch 0060 - Avg Loss: 0.2023\n",
            "[Epoch 15] Batch 0070 - Avg Loss: 0.1829\n",
            "[Epoch 15] Batch 0080 - Avg Loss: 0.1601\n",
            "[Epoch 15] Batch 0090 - Avg Loss: 0.1673\n",
            "[Epoch 15] Batch 0100 - Avg Loss: 0.1807\n",
            "[Epoch 15] Batch 0110 - Avg Loss: 0.2350\n",
            "[Epoch 15] Batch 0120 - Avg Loss: 0.2111\n",
            "[Epoch 15] Batch 0130 - Avg Loss: 0.1593\n",
            "[Epoch 15] Batch 0140 - Avg Loss: 0.1665\n",
            "[Epoch 15] Batch 0150 - Avg Loss: 0.1759\n",
            "[Epoch 15] Batch 0160 - Avg Loss: 0.1591\n",
            "[Epoch 15] Batch 0170 - Avg Loss: 0.2038\n",
            "[Epoch 15] Batch 0180 - Avg Loss: 0.2193\n",
            "[Epoch 15] Batch 0190 - Avg Loss: 0.2115\n",
            "[Epoch 15] Batch 0200 - Avg Loss: 0.2276\n",
            "[Epoch 15] Batch 0210 - Avg Loss: 0.1486\n",
            "[Epoch 15] Batch 0220 - Avg Loss: 0.1468\n",
            "[Epoch 15] Batch 0230 - Avg Loss: 0.1737\n",
            "[Epoch 15] Batch 0240 - Avg Loss: 0.1698\n",
            "[Epoch 15] Batch 0250 - Avg Loss: 0.2155\n",
            "[Epoch 15] Batch 0260 - Avg Loss: 0.1940\n",
            "[Epoch 15] Batch 0270 - Avg Loss: 0.1823\n",
            "[Epoch 15] Batch 0280 - Avg Loss: 0.2026\n",
            "[Epoch 15] Batch 0290 - Avg Loss: 0.1830\n",
            "[Epoch 15] Batch 0300 - Avg Loss: 0.2010\n",
            "[Epoch 15] Batch 0310 - Avg Loss: 0.2102\n",
            "[Epoch 15] Batch 0320 - Avg Loss: 0.2107\n",
            "[Epoch 15] Batch 0330 - Avg Loss: 0.2462\n",
            "\n",
            " Val Acc: 94.24%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.93      0.94      0.94       600\n",
            "              Forest       0.97      0.99      0.98       600\n",
            "HerbaceousVegetation       0.90      0.87      0.89       600\n",
            "             Highway       0.96      0.90      0.93       500\n",
            "          Industrial       0.94      0.95      0.95       500\n",
            "             Pasture       0.90      0.96      0.93       400\n",
            "       PermanentCrop       0.86      0.87      0.86       500\n",
            "         Residential       0.96      0.98      0.97       600\n",
            "               River       0.98      0.97      0.97       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.94      5400\n",
            "           macro avg       0.94      0.94      0.94      5400\n",
            "        weighted avg       0.94      0.94      0.94      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1916, Acc: 93.72\n",
            "Val Acc: 94.24\n",
            " Saved new best model: .//best_model_epoch_15.pth\n",
            "[Epoch 16] Batch 0010 - Avg Loss: 0.2143\n",
            "[Epoch 16] Batch 0020 - Avg Loss: 0.1719\n",
            "[Epoch 16] Batch 0030 - Avg Loss: 0.1587\n",
            "[Epoch 16] Batch 0040 - Avg Loss: 0.1849\n",
            "[Epoch 16] Batch 0050 - Avg Loss: 0.1502\n",
            "[Epoch 16] Batch 0060 - Avg Loss: 0.1626\n",
            "[Epoch 16] Batch 0070 - Avg Loss: 0.2117\n",
            "[Epoch 16] Batch 0080 - Avg Loss: 0.1926\n",
            "[Epoch 16] Batch 0090 - Avg Loss: 0.1733\n",
            "[Epoch 16] Batch 0100 - Avg Loss: 0.1906\n",
            "[Epoch 16] Batch 0110 - Avg Loss: 0.2047\n",
            "[Epoch 16] Batch 0120 - Avg Loss: 0.2034\n",
            "[Epoch 16] Batch 0130 - Avg Loss: 0.1504\n",
            "[Epoch 16] Batch 0140 - Avg Loss: 0.1740\n",
            "[Epoch 16] Batch 0150 - Avg Loss: 0.1745\n",
            "[Epoch 16] Batch 0160 - Avg Loss: 0.1733\n",
            "[Epoch 16] Batch 0170 - Avg Loss: 0.2120\n",
            "[Epoch 16] Batch 0180 - Avg Loss: 0.2189\n",
            "[Epoch 16] Batch 0190 - Avg Loss: 0.1720\n",
            "[Epoch 16] Batch 0200 - Avg Loss: 0.1631\n",
            "[Epoch 16] Batch 0210 - Avg Loss: 0.2039\n",
            "[Epoch 16] Batch 0220 - Avg Loss: 0.2016\n",
            "[Epoch 16] Batch 0230 - Avg Loss: 0.1869\n",
            "[Epoch 16] Batch 0240 - Avg Loss: 0.1959\n",
            "[Epoch 16] Batch 0250 - Avg Loss: 0.2005\n",
            "[Epoch 16] Batch 0260 - Avg Loss: 0.1580\n",
            "[Epoch 16] Batch 0270 - Avg Loss: 0.1910\n",
            "[Epoch 16] Batch 0280 - Avg Loss: 0.1533\n",
            "[Epoch 16] Batch 0290 - Avg Loss: 0.1733\n",
            "[Epoch 16] Batch 0300 - Avg Loss: 0.1332\n",
            "[Epoch 16] Batch 0310 - Avg Loss: 0.1953\n",
            "[Epoch 16] Batch 0320 - Avg Loss: 0.1889\n",
            "[Epoch 16] Batch 0330 - Avg Loss: 0.2108\n",
            "\n",
            " Val Acc: 94.70%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.94      0.94      0.94       600\n",
            "              Forest       0.98      0.97      0.98       600\n",
            "HerbaceousVegetation       0.92      0.86      0.89       600\n",
            "             Highway       0.94      0.94      0.94       500\n",
            "          Industrial       0.95      0.98      0.96       500\n",
            "             Pasture       0.90      0.95      0.92       400\n",
            "       PermanentCrop       0.88      0.87      0.88       500\n",
            "         Residential       0.96      0.99      0.97       600\n",
            "               River       0.98      0.97      0.98       500\n",
            "             SeaLake       0.99      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.95      5400\n",
            "           macro avg       0.94      0.95      0.95      5400\n",
            "        weighted avg       0.95      0.95      0.95      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1839, Acc: 93.72\n",
            "Val Acc: 94.70\n",
            " Saved new best model: .//best_model_epoch_16.pth\n",
            "[Epoch 17] Batch 0010 - Avg Loss: 0.1739\n",
            "[Epoch 17] Batch 0020 - Avg Loss: 0.1948\n",
            "[Epoch 17] Batch 0030 - Avg Loss: 0.1581\n",
            "[Epoch 17] Batch 0040 - Avg Loss: 0.1732\n",
            "[Epoch 17] Batch 0050 - Avg Loss: 0.1695\n",
            "[Epoch 17] Batch 0060 - Avg Loss: 0.1352\n",
            "[Epoch 17] Batch 0070 - Avg Loss: 0.1406\n",
            "[Epoch 17] Batch 0080 - Avg Loss: 0.1371\n",
            "[Epoch 17] Batch 0090 - Avg Loss: 0.1534\n",
            "[Epoch 17] Batch 0100 - Avg Loss: 0.1196\n",
            "[Epoch 17] Batch 0110 - Avg Loss: 0.1931\n",
            "[Epoch 17] Batch 0120 - Avg Loss: 0.1677\n",
            "[Epoch 17] Batch 0130 - Avg Loss: 0.1491\n",
            "[Epoch 17] Batch 0140 - Avg Loss: 0.1819\n",
            "[Epoch 17] Batch 0150 - Avg Loss: 0.1710\n",
            "[Epoch 17] Batch 0160 - Avg Loss: 0.1694\n",
            "[Epoch 17] Batch 0170 - Avg Loss: 0.1380\n",
            "[Epoch 17] Batch 0180 - Avg Loss: 0.1779\n",
            "[Epoch 17] Batch 0190 - Avg Loss: 0.1609\n",
            "[Epoch 17] Batch 0200 - Avg Loss: 0.1608\n",
            "[Epoch 17] Batch 0210 - Avg Loss: 0.2109\n",
            "[Epoch 17] Batch 0220 - Avg Loss: 0.1699\n",
            "[Epoch 17] Batch 0230 - Avg Loss: 0.1976\n",
            "[Epoch 17] Batch 0240 - Avg Loss: 0.1672\n",
            "[Epoch 17] Batch 0250 - Avg Loss: 0.1903\n",
            "[Epoch 17] Batch 0260 - Avg Loss: 0.1273\n",
            "[Epoch 17] Batch 0270 - Avg Loss: 0.1353\n",
            "[Epoch 17] Batch 0280 - Avg Loss: 0.1480\n",
            "[Epoch 17] Batch 0290 - Avg Loss: 0.1687\n",
            "[Epoch 17] Batch 0300 - Avg Loss: 0.2181\n",
            "[Epoch 17] Batch 0310 - Avg Loss: 0.1610\n",
            "[Epoch 17] Batch 0320 - Avg Loss: 0.1723\n",
            "[Epoch 17] Batch 0330 - Avg Loss: 0.1915\n",
            "\n",
            " Val Acc: 94.81%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.93      0.93      0.93       600\n",
            "              Forest       0.99      0.98      0.98       600\n",
            "HerbaceousVegetation       0.89      0.92      0.91       600\n",
            "             Highway       0.95      0.91      0.93       500\n",
            "          Industrial       0.99      0.94      0.96       500\n",
            "             Pasture       0.89      0.95      0.92       400\n",
            "       PermanentCrop       0.90      0.87      0.89       500\n",
            "         Residential       0.97      0.98      0.98       600\n",
            "               River       0.95      0.98      0.97       500\n",
            "             SeaLake       0.99      1.00      0.99       600\n",
            "\n",
            "            accuracy                           0.95      5400\n",
            "           macro avg       0.95      0.95      0.95      5400\n",
            "        weighted avg       0.95      0.95      0.95      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1659, Acc: 94.46\n",
            "Val Acc: 94.81\n",
            " Saved new best model: .//best_model_epoch_17.pth\n",
            "[Epoch 18] Batch 0010 - Avg Loss: 0.1412\n",
            "[Epoch 18] Batch 0020 - Avg Loss: 0.1707\n",
            "[Epoch 18] Batch 0030 - Avg Loss: 0.2040\n",
            "[Epoch 18] Batch 0040 - Avg Loss: 0.1553\n",
            "[Epoch 18] Batch 0050 - Avg Loss: 0.1521\n",
            "[Epoch 18] Batch 0060 - Avg Loss: 0.1684\n",
            "[Epoch 18] Batch 0070 - Avg Loss: 0.1551\n",
            "[Epoch 18] Batch 0080 - Avg Loss: 0.1420\n",
            "[Epoch 18] Batch 0090 - Avg Loss: 0.1220\n",
            "[Epoch 18] Batch 0100 - Avg Loss: 0.1176\n",
            "[Epoch 18] Batch 0110 - Avg Loss: 0.1859\n",
            "[Epoch 18] Batch 0120 - Avg Loss: 0.1413\n",
            "[Epoch 18] Batch 0130 - Avg Loss: 0.1250\n",
            "[Epoch 18] Batch 0140 - Avg Loss: 0.1988\n",
            "[Epoch 18] Batch 0150 - Avg Loss: 0.1399\n",
            "[Epoch 18] Batch 0160 - Avg Loss: 0.1994\n",
            "[Epoch 18] Batch 0170 - Avg Loss: 0.1699\n",
            "[Epoch 18] Batch 0180 - Avg Loss: 0.1525\n",
            "[Epoch 18] Batch 0190 - Avg Loss: 0.1889\n",
            "[Epoch 18] Batch 0200 - Avg Loss: 0.1685\n",
            "[Epoch 18] Batch 0210 - Avg Loss: 0.1685\n",
            "[Epoch 18] Batch 0220 - Avg Loss: 0.1534\n",
            "[Epoch 18] Batch 0230 - Avg Loss: 0.1364\n",
            "[Epoch 18] Batch 0240 - Avg Loss: 0.1246\n",
            "[Epoch 18] Batch 0250 - Avg Loss: 0.1674\n",
            "[Epoch 18] Batch 0260 - Avg Loss: 0.1672\n",
            "[Epoch 18] Batch 0270 - Avg Loss: 0.1897\n",
            "[Epoch 18] Batch 0280 - Avg Loss: 0.1803\n",
            "[Epoch 18] Batch 0290 - Avg Loss: 0.1900\n",
            "[Epoch 18] Batch 0300 - Avg Loss: 0.1776\n",
            "[Epoch 18] Batch 0310 - Avg Loss: 0.1771\n",
            "[Epoch 18] Batch 0320 - Avg Loss: 0.1707\n",
            "[Epoch 18] Batch 0330 - Avg Loss: 0.1896\n",
            "\n",
            " Val Acc: 94.78%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.94      0.94      0.94       600\n",
            "              Forest       0.99      0.96      0.98       600\n",
            "HerbaceousVegetation       0.88      0.92      0.90       600\n",
            "             Highway       0.92      0.95      0.93       500\n",
            "          Industrial       0.98      0.96      0.97       500\n",
            "             Pasture       0.93      0.93      0.93       400\n",
            "       PermanentCrop       0.90      0.86      0.88       500\n",
            "         Residential       0.98      0.98      0.98       600\n",
            "               River       0.98      0.98      0.98       500\n",
            "             SeaLake       0.98      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.95      5400\n",
            "           macro avg       0.95      0.95      0.95      5400\n",
            "        weighted avg       0.95      0.95      0.95      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1644, Acc: 94.45\n",
            "Val Acc: 94.78\n",
            "[Epoch 19] Batch 0010 - Avg Loss: 0.1215\n",
            "[Epoch 19] Batch 0020 - Avg Loss: 0.1396\n",
            "[Epoch 19] Batch 0030 - Avg Loss: 0.1525\n",
            "[Epoch 19] Batch 0040 - Avg Loss: 0.1362\n",
            "[Epoch 19] Batch 0050 - Avg Loss: 0.1840\n",
            "[Epoch 19] Batch 0060 - Avg Loss: 0.1473\n",
            "[Epoch 19] Batch 0070 - Avg Loss: 0.1888\n",
            "[Epoch 19] Batch 0080 - Avg Loss: 0.1366\n",
            "[Epoch 19] Batch 0090 - Avg Loss: 0.1320\n",
            "[Epoch 19] Batch 0100 - Avg Loss: 0.1419\n",
            "[Epoch 19] Batch 0110 - Avg Loss: 0.1384\n",
            "[Epoch 19] Batch 0120 - Avg Loss: 0.2144\n",
            "[Epoch 19] Batch 0130 - Avg Loss: 0.2154\n",
            "[Epoch 19] Batch 0140 - Avg Loss: 0.1402\n",
            "[Epoch 19] Batch 0150 - Avg Loss: 0.1438\n",
            "[Epoch 19] Batch 0160 - Avg Loss: 0.1673\n",
            "[Epoch 19] Batch 0170 - Avg Loss: 0.1559\n",
            "[Epoch 19] Batch 0180 - Avg Loss: 0.1455\n",
            "[Epoch 19] Batch 0190 - Avg Loss: 0.1430\n",
            "[Epoch 19] Batch 0200 - Avg Loss: 0.1635\n",
            "[Epoch 19] Batch 0210 - Avg Loss: 0.1339\n",
            "[Epoch 19] Batch 0220 - Avg Loss: 0.1275\n",
            "[Epoch 19] Batch 0230 - Avg Loss: 0.1922\n",
            "[Epoch 19] Batch 0240 - Avg Loss: 0.1802\n",
            "[Epoch 19] Batch 0250 - Avg Loss: 0.1890\n",
            "[Epoch 19] Batch 0260 - Avg Loss: 0.1285\n",
            "[Epoch 19] Batch 0270 - Avg Loss: 0.1947\n",
            "[Epoch 19] Batch 0280 - Avg Loss: 0.1569\n",
            "[Epoch 19] Batch 0290 - Avg Loss: 0.1753\n",
            "[Epoch 19] Batch 0300 - Avg Loss: 0.1503\n",
            "[Epoch 19] Batch 0310 - Avg Loss: 0.1599\n",
            "[Epoch 19] Batch 0320 - Avg Loss: 0.2484\n",
            "[Epoch 19] Batch 0330 - Avg Loss: 0.1154\n",
            "\n",
            " Val Acc: 95.06%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.97      0.93      0.95       600\n",
            "              Forest       0.97      0.99      0.98       600\n",
            "HerbaceousVegetation       0.89      0.92      0.91       600\n",
            "             Highway       0.96      0.92      0.94       500\n",
            "          Industrial       0.95      0.97      0.96       500\n",
            "             Pasture       0.94      0.92      0.93       400\n",
            "       PermanentCrop       0.91      0.86      0.89       500\n",
            "         Residential       0.97      0.97      0.97       600\n",
            "               River       0.94      0.99      0.97       500\n",
            "             SeaLake       0.99      1.00      0.99       600\n",
            "\n",
            "            accuracy                           0.95      5400\n",
            "           macro avg       0.95      0.95      0.95      5400\n",
            "        weighted avg       0.95      0.95      0.95      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1586, Acc: 94.71\n",
            "Val Acc: 95.06\n",
            " Saved new best model: .//best_model_epoch_19.pth\n",
            "[Epoch 20] Batch 0010 - Avg Loss: 0.1494\n",
            "[Epoch 20] Batch 0020 - Avg Loss: 0.1565\n",
            "[Epoch 20] Batch 0030 - Avg Loss: 0.1752\n",
            "[Epoch 20] Batch 0040 - Avg Loss: 0.1684\n",
            "[Epoch 20] Batch 0050 - Avg Loss: 0.1568\n",
            "[Epoch 20] Batch 0060 - Avg Loss: 0.1610\n",
            "[Epoch 20] Batch 0070 - Avg Loss: 0.1403\n",
            "[Epoch 20] Batch 0080 - Avg Loss: 0.1365\n",
            "[Epoch 20] Batch 0090 - Avg Loss: 0.2198\n",
            "[Epoch 20] Batch 0100 - Avg Loss: 0.1851\n",
            "[Epoch 20] Batch 0110 - Avg Loss: 0.1901\n",
            "[Epoch 20] Batch 0120 - Avg Loss: 0.1287\n",
            "[Epoch 20] Batch 0130 - Avg Loss: 0.1442\n",
            "[Epoch 20] Batch 0140 - Avg Loss: 0.1990\n",
            "[Epoch 20] Batch 0150 - Avg Loss: 0.1224\n",
            "[Epoch 20] Batch 0160 - Avg Loss: 0.1314\n",
            "[Epoch 20] Batch 0170 - Avg Loss: 0.1593\n",
            "[Epoch 20] Batch 0180 - Avg Loss: 0.1511\n",
            "[Epoch 20] Batch 0190 - Avg Loss: 0.1483\n",
            "[Epoch 20] Batch 0200 - Avg Loss: 0.1309\n",
            "[Epoch 20] Batch 0210 - Avg Loss: 0.1541\n",
            "[Epoch 20] Batch 0220 - Avg Loss: 0.1331\n",
            "[Epoch 20] Batch 0230 - Avg Loss: 0.1454\n",
            "[Epoch 20] Batch 0240 - Avg Loss: 0.1188\n",
            "[Epoch 20] Batch 0250 - Avg Loss: 0.1599\n",
            "[Epoch 20] Batch 0260 - Avg Loss: 0.1549\n",
            "[Epoch 20] Batch 0270 - Avg Loss: 0.1283\n",
            "[Epoch 20] Batch 0280 - Avg Loss: 0.1451\n",
            "[Epoch 20] Batch 0290 - Avg Loss: 0.1723\n",
            "[Epoch 20] Batch 0300 - Avg Loss: 0.1602\n",
            "[Epoch 20] Batch 0310 - Avg Loss: 0.1526\n",
            "[Epoch 20] Batch 0320 - Avg Loss: 0.1570\n",
            "[Epoch 20] Batch 0330 - Avg Loss: 0.1938\n",
            "\n",
            " Val Acc: 95.35%\n",
            "\n",
            " Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.95      0.95      0.95       600\n",
            "              Forest       0.99      0.98      0.99       600\n",
            "HerbaceousVegetation       0.89      0.93      0.91       600\n",
            "             Highway       0.96      0.93      0.95       500\n",
            "          Industrial       0.96      0.96      0.96       500\n",
            "             Pasture       0.92      0.95      0.93       400\n",
            "       PermanentCrop       0.90      0.89      0.90       500\n",
            "         Residential       0.98      0.98      0.98       600\n",
            "               River       0.99      0.96      0.97       500\n",
            "             SeaLake       1.00      0.99      0.99       600\n",
            "\n",
            "            accuracy                           0.95      5400\n",
            "           macro avg       0.95      0.95      0.95      5400\n",
            "        weighted avg       0.95      0.95      0.95      5400\n",
            "\n",
            "\n",
            "Train Loss: 0.1556, Acc: 94.75\n",
            "Val Acc: 95.35\n",
            " Saved new best model: .//best_model_epoch_20.pth\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "run_training(train_loader, val_loader, df, num_epochs=20, save_path=\"./\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etkiLal_Wbu3"
      },
      "source": [
        "**# Inference + Submission**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0dfHE3PWbu3"
      },
      "source": [
        "# **1. Load Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rbWdAJwHWbu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f953203-a8f3-47b0-bc17-cf95f4562f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing keys : ['model.conv1.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.layer1.0.conv1.weight', 'model.layer1.0.bn1.weight', 'model.layer1.0.bn1.bias', 'model.layer1.0.bn1.running_mean', 'model.layer1.0.bn1.running_var', 'model.layer1.0.conv2.weight', 'model.layer1.0.bn2.weight', 'model.layer1.0.bn2.bias', 'model.layer1.0.bn2.running_mean', 'model.layer1.0.bn2.running_var', 'model.layer1.0.conv3.weight', 'model.layer1.0.bn3.weight', 'model.layer1.0.bn3.bias', 'model.layer1.0.bn3.running_mean', 'model.layer1.0.bn3.running_var', 'model.layer1.0.downsample.0.weight', 'model.layer1.0.downsample.1.weight', 'model.layer1.0.downsample.1.bias', 'model.layer1.0.downsample.1.running_mean', 'model.layer1.0.downsample.1.running_var', 'model.layer1.1.conv1.weight', 'model.layer1.1.bn1.weight', 'model.layer1.1.bn1.bias', 'model.layer1.1.bn1.running_mean', 'model.layer1.1.bn1.running_var', 'model.layer1.1.conv2.weight', 'model.layer1.1.bn2.weight', 'model.layer1.1.bn2.bias', 'model.layer1.1.bn2.running_mean', 'model.layer1.1.bn2.running_var', 'model.layer1.1.conv3.weight', 'model.layer1.1.bn3.weight', 'model.layer1.1.bn3.bias', 'model.layer1.1.bn3.running_mean', 'model.layer1.1.bn3.running_var', 'model.layer1.2.conv1.weight', 'model.layer1.2.bn1.weight', 'model.layer1.2.bn1.bias', 'model.layer1.2.bn1.running_mean', 'model.layer1.2.bn1.running_var', 'model.layer1.2.conv2.weight', 'model.layer1.2.bn2.weight', 'model.layer1.2.bn2.bias', 'model.layer1.2.bn2.running_mean', 'model.layer1.2.bn2.running_var', 'model.layer1.2.conv3.weight', 'model.layer1.2.bn3.weight', 'model.layer1.2.bn3.bias', 'model.layer1.2.bn3.running_mean', 'model.layer1.2.bn3.running_var', 'model.layer2.0.conv1.weight', 'model.layer2.0.bn1.weight', 'model.layer2.0.bn1.bias', 'model.layer2.0.bn1.running_mean', 'model.layer2.0.bn1.running_var', 'model.layer2.0.conv2.weight', 'model.layer2.0.bn2.weight', 'model.layer2.0.bn2.bias', 'model.layer2.0.bn2.running_mean', 'model.layer2.0.bn2.running_var', 'model.layer2.0.conv3.weight', 'model.layer2.0.bn3.weight', 'model.layer2.0.bn3.bias', 'model.layer2.0.bn3.running_mean', 'model.layer2.0.bn3.running_var', 'model.layer2.0.downsample.0.weight', 'model.layer2.0.downsample.1.weight', 'model.layer2.0.downsample.1.bias', 'model.layer2.0.downsample.1.running_mean', 'model.layer2.0.downsample.1.running_var', 'model.layer2.1.conv1.weight', 'model.layer2.1.bn1.weight', 'model.layer2.1.bn1.bias', 'model.layer2.1.bn1.running_mean', 'model.layer2.1.bn1.running_var', 'model.layer2.1.conv2.weight', 'model.layer2.1.bn2.weight', 'model.layer2.1.bn2.bias', 'model.layer2.1.bn2.running_mean', 'model.layer2.1.bn2.running_var', 'model.layer2.1.conv3.weight', 'model.layer2.1.bn3.weight', 'model.layer2.1.bn3.bias', 'model.layer2.1.bn3.running_mean', 'model.layer2.1.bn3.running_var', 'model.layer2.2.conv1.weight', 'model.layer2.2.bn1.weight', 'model.layer2.2.bn1.bias', 'model.layer2.2.bn1.running_mean', 'model.layer2.2.bn1.running_var', 'model.layer2.2.conv2.weight', 'model.layer2.2.bn2.weight', 'model.layer2.2.bn2.bias', 'model.layer2.2.bn2.running_mean', 'model.layer2.2.bn2.running_var', 'model.layer2.2.conv3.weight', 'model.layer2.2.bn3.weight', 'model.layer2.2.bn3.bias', 'model.layer2.2.bn3.running_mean', 'model.layer2.2.bn3.running_var', 'model.layer2.3.conv1.weight', 'model.layer2.3.bn1.weight', 'model.layer2.3.bn1.bias', 'model.layer2.3.bn1.running_mean', 'model.layer2.3.bn1.running_var', 'model.layer2.3.conv2.weight', 'model.layer2.3.bn2.weight', 'model.layer2.3.bn2.bias', 'model.layer2.3.bn2.running_mean', 'model.layer2.3.bn2.running_var', 'model.layer2.3.conv3.weight', 'model.layer2.3.bn3.weight', 'model.layer2.3.bn3.bias', 'model.layer2.3.bn3.running_mean', 'model.layer2.3.bn3.running_var', 'model.layer3.0.conv1.weight', 'model.layer3.0.bn1.weight', 'model.layer3.0.bn1.bias', 'model.layer3.0.bn1.running_mean', 'model.layer3.0.bn1.running_var', 'model.layer3.0.conv2.weight', 'model.layer3.0.bn2.weight', 'model.layer3.0.bn2.bias', 'model.layer3.0.bn2.running_mean', 'model.layer3.0.bn2.running_var', 'model.layer3.0.conv3.weight', 'model.layer3.0.bn3.weight', 'model.layer3.0.bn3.bias', 'model.layer3.0.bn3.running_mean', 'model.layer3.0.bn3.running_var', 'model.layer3.0.downsample.0.weight', 'model.layer3.0.downsample.1.weight', 'model.layer3.0.downsample.1.bias', 'model.layer3.0.downsample.1.running_mean', 'model.layer3.0.downsample.1.running_var', 'model.layer3.1.conv1.weight', 'model.layer3.1.bn1.weight', 'model.layer3.1.bn1.bias', 'model.layer3.1.bn1.running_mean', 'model.layer3.1.bn1.running_var', 'model.layer3.1.conv2.weight', 'model.layer3.1.bn2.weight', 'model.layer3.1.bn2.bias', 'model.layer3.1.bn2.running_mean', 'model.layer3.1.bn2.running_var', 'model.layer3.1.conv3.weight', 'model.layer3.1.bn3.weight', 'model.layer3.1.bn3.bias', 'model.layer3.1.bn3.running_mean', 'model.layer3.1.bn3.running_var', 'model.layer3.2.conv1.weight', 'model.layer3.2.bn1.weight', 'model.layer3.2.bn1.bias', 'model.layer3.2.bn1.running_mean', 'model.layer3.2.bn1.running_var', 'model.layer3.2.conv2.weight', 'model.layer3.2.bn2.weight', 'model.layer3.2.bn2.bias', 'model.layer3.2.bn2.running_mean', 'model.layer3.2.bn2.running_var', 'model.layer3.2.conv3.weight', 'model.layer3.2.bn3.weight', 'model.layer3.2.bn3.bias', 'model.layer3.2.bn3.running_mean', 'model.layer3.2.bn3.running_var', 'model.layer3.3.conv1.weight', 'model.layer3.3.bn1.weight', 'model.layer3.3.bn1.bias', 'model.layer3.3.bn1.running_mean', 'model.layer3.3.bn1.running_var', 'model.layer3.3.conv2.weight', 'model.layer3.3.bn2.weight', 'model.layer3.3.bn2.bias', 'model.layer3.3.bn2.running_mean', 'model.layer3.3.bn2.running_var', 'model.layer3.3.conv3.weight', 'model.layer3.3.bn3.weight', 'model.layer3.3.bn3.bias', 'model.layer3.3.bn3.running_mean', 'model.layer3.3.bn3.running_var', 'model.layer3.4.conv1.weight', 'model.layer3.4.bn1.weight', 'model.layer3.4.bn1.bias', 'model.layer3.4.bn1.running_mean', 'model.layer3.4.bn1.running_var', 'model.layer3.4.conv2.weight', 'model.layer3.4.bn2.weight', 'model.layer3.4.bn2.bias', 'model.layer3.4.bn2.running_mean', 'model.layer3.4.bn2.running_var', 'model.layer3.4.conv3.weight', 'model.layer3.4.bn3.weight', 'model.layer3.4.bn3.bias', 'model.layer3.4.bn3.running_mean', 'model.layer3.4.bn3.running_var', 'model.layer3.5.conv1.weight', 'model.layer3.5.bn1.weight', 'model.layer3.5.bn1.bias', 'model.layer3.5.bn1.running_mean', 'model.layer3.5.bn1.running_var', 'model.layer3.5.conv2.weight', 'model.layer3.5.bn2.weight', 'model.layer3.5.bn2.bias', 'model.layer3.5.bn2.running_mean', 'model.layer3.5.bn2.running_var', 'model.layer3.5.conv3.weight', 'model.layer3.5.bn3.weight', 'model.layer3.5.bn3.bias', 'model.layer3.5.bn3.running_mean', 'model.layer3.5.bn3.running_var', 'model.layer4.0.conv1.weight', 'model.layer4.0.bn1.weight', 'model.layer4.0.bn1.bias', 'model.layer4.0.bn1.running_mean', 'model.layer4.0.bn1.running_var', 'model.layer4.0.conv2.weight', 'model.layer4.0.bn2.weight', 'model.layer4.0.bn2.bias', 'model.layer4.0.bn2.running_mean', 'model.layer4.0.bn2.running_var', 'model.layer4.0.conv3.weight', 'model.layer4.0.bn3.weight', 'model.layer4.0.bn3.bias', 'model.layer4.0.bn3.running_mean', 'model.layer4.0.bn3.running_var', 'model.layer4.0.downsample.0.weight', 'model.layer4.0.downsample.1.weight', 'model.layer4.0.downsample.1.bias', 'model.layer4.0.downsample.1.running_mean', 'model.layer4.0.downsample.1.running_var', 'model.layer4.1.conv1.weight', 'model.layer4.1.bn1.weight', 'model.layer4.1.bn1.bias', 'model.layer4.1.bn1.running_mean', 'model.layer4.1.bn1.running_var', 'model.layer4.1.conv2.weight', 'model.layer4.1.bn2.weight', 'model.layer4.1.bn2.bias', 'model.layer4.1.bn2.running_mean', 'model.layer4.1.bn2.running_var', 'model.layer4.1.conv3.weight', 'model.layer4.1.bn3.weight', 'model.layer4.1.bn3.bias', 'model.layer4.1.bn3.running_mean', 'model.layer4.1.bn3.running_var', 'model.layer4.2.conv1.weight', 'model.layer4.2.bn1.weight', 'model.layer4.2.bn1.bias', 'model.layer4.2.bn1.running_mean', 'model.layer4.2.bn1.running_var', 'model.layer4.2.conv2.weight', 'model.layer4.2.bn2.weight', 'model.layer4.2.bn2.bias', 'model.layer4.2.bn2.running_mean', 'model.layer4.2.bn2.running_var', 'model.layer4.2.conv3.weight', 'model.layer4.2.bn3.weight', 'model.layer4.2.bn3.bias', 'model.layer4.2.bn3.running_mean', 'model.layer4.2.bn3.running_var', 'model.fc.weight', 'model.fc.bias']\n",
            "Unexpected   : ['model_state', 'class_to_idx']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet50_MultiChannel(\n",
              "  (model): ResNet(\n",
              "    (conv1): Conv2d(16, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "best_model_path = \"/content/best_model_epoch_20.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1️⃣  Load the checkpoint\n",
        "ckpt = torch.load(best_model_path, map_location=device)\n",
        "\n",
        "# 2️⃣  Get the actual weight dict\n",
        "state_dict = ckpt.get(\"state_dict\", ckpt)          # Lightning-style or plain\n",
        "\n",
        "# 3️⃣  Clean up the keys\n",
        "clean_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith(\"module.\"): k = k[7:]          # drop 'module.'\n",
        "    if k.startswith(\"model.\"):  k = k[6:]          # drop 'model.'\n",
        "    clean_state_dict[k] = v\n",
        "\n",
        "# 4️⃣  Build the model and load weights (non-strict so extras are ignored)\n",
        "model = ResNet50_MultiChannel(num_classes=df[\"label_idx\"].nunique())\n",
        "missing, unexpected = model.load_state_dict(clean_state_dict, strict=False)\n",
        "\n",
        "print(\"Missing keys :\", missing)      # should be [] if arch is identical\n",
        "print(\"Unexpected   :\", unexpected)   # safely ignored\n",
        "\n",
        "model.to(device).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un6lRd63Wbu3"
      },
      "source": [
        "# **2. Build Class Index → Label Mapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DV0MdtkUgQ0P"
      },
      "outputs": [],
      "source": [
        "!unzip -q testset.zip -d TestSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "25h5oW-CWbu3"
      },
      "outputs": [],
      "source": [
        "##idx_to_class = {v: k for k, v in df[['label', 'label_idx']].drop_duplicates().set_index('label').to_dict()['label_idx'].items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ1UR07uWbu3"
      },
      "source": [
        "# **3. Inference on Test NPY Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pQTpjT19Wbu3"
      },
      "outputs": [],
      "source": [
        "test_dir = \"/content/TestSet/testset/testset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MfYnyiChWbu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4922c74-7ac1-4dd8-9142-d4b66be90448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: True\n",
            "Files found: 4232\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "print(\"Exists:\", os.path.exists(test_dir))\n",
        "print(\"Files found:\", len(glob(os.path.join(test_dir, \"*.npy\"))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G0kHbkVPWbu3"
      },
      "outputs": [],
      "source": [
        "#mean = torch.tensor([0.1821, 0.2301, 0.0946, 0.3486], device=device).view(4, 1, 1)\n",
        "#std = torch.tensor([0.1001, 0.1117, 0.0592, 0.3300], device=device).view(4, 1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_xcyH62MWbu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1144977-356e-431f-d48d-6a9975ce22cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Inference: 100%|██████████| 4232/4232 [00:42<00:00, 100.18it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_dir = \"/content/TestSet/testset/testset\"\n",
        "ckpt = torch.load(\"/content/best_model_epoch_20.pth\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "idx_to_class = ckpt[\"class_to_idx\"]      # <-- guaranteed to match training\n",
        "model.eval()\n",
        "test_files = sorted(glob(os.path.join(test_dir, \"*.npy\")))\n",
        "\n",
        "#mean = torch.tensor([0.1821, 0.2301, 0.0946, 0.3486], device=device).view(4, 1, 1)\n",
        "#std = torch.tensor([0.1001, 0.1117, 0.0592, 0.3300], device=device).view(4, 1, 1)\n",
        "\n",
        "results = []\n",
        "\n",
        "def process_test_data(path):\n",
        "    x = np.load(path).astype(np.float32)\n",
        "    x = np.transpose(x, (2, 0, 1))\n",
        "    # No transpose needed, assuming the input is (C, H, W) after loading\n",
        "\n",
        "    # Raw bands (scaled to 0-1)\n",
        "    selected_indices = [1,2,3,4,5,6,7,9,10,11,8]   # B1-B9, B11-B12\n",
        "    raw_bands = x[selected_indices] / 10000.0\n",
        "\n",
        "    # Indices\n",
        "    B3, B4, B8, B11 = x[2], x[3], x[7], x[11]   # note: indices w.r.t selected set\n",
        "    ndvi = (B8 - B4) / (B8 + B4 + 1e-6)\n",
        "    ndwi = (B3 - B8) / (B3 + B8 + 1e-6)\n",
        "    ndbi = (B11 - B8) / (B11 + B8 + 1e-6)\n",
        "    ndmi = (B8 - B11) / (B8 + B11 + 1e-6)\n",
        "    savi = (1.5 * (B8 - B4)) / (B8 + B4 + 0.5)\n",
        "\n",
        "\n",
        "    indices = np.stack([ndvi, ndwi, ndbi, ndmi, savi], axis=0)\n",
        "    indices = np.clip(indices, -1, 1)\n",
        "\n",
        "    x_final = np.concatenate([raw_bands, indices], axis=0).astype(np.float32)\n",
        "    x_final = torch.tensor(x_final)\n",
        "\n",
        "    #x_final = torch.nn.functional.interpolate(\n",
        "     #       torch.tensor(x_final).unsqueeze(0),\n",
        "      #      size=(64, 64),\n",
        "       #     mode=\"bilinear\",\n",
        "        #    align_corners=False\n",
        "         # ).squeeze(0)\n",
        "\n",
        "    # Calculate mean and std per sample\n",
        "    mean = x_final.mean(dim=[1, 2], keepdim=True)\n",
        "    std = x_final.std(dim=[1, 2], keepdim=True)\n",
        "\n",
        "    # Normalize the sample\n",
        "    x_final = (x_final - mean) / (std + 1e-5)  # Adding a small constant to avoid division by zero\n",
        "\n",
        "    return x_final\n",
        "\n",
        "\n",
        "# Update inference loop\n",
        "with torch.no_grad():\n",
        "    for path in tqdm(test_files, desc=\"Running Inference\"):\n",
        "        x_final = process_test_data(path)\n",
        "        x_final = x_final.to(device, dtype=torch.float32)\n",
        "        x_final = x_final.unsqueeze(0)\n",
        "\n",
        "        out = model(x_final)\n",
        "        pred = out.argmax(1).item()\n",
        "        label = idx_to_class[pred]\n",
        "        results.append((os.path.basename(path).replace(\".npy\", \"\"), label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD8YKLxsWbu4"
      },
      "source": [
        "# **4. Generate Submission File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yTCj7Yu7Wbu4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 'results' contains tuples like: [('test_0', 'Forest'), ('test_1', 'River'), ...]\n",
        "\n",
        "# Convert to DataFrame with correct format\n",
        "submission_df = pd.DataFrame(results, columns=[\"test_id\", \"label\"])\n",
        "\n",
        "# Extract numeric part of test_id and convert to int\n",
        "submission_df[\"test_id\"] = submission_df[\"test_id\"].str.extract(r\"(\\d+)\").astype(int)\n",
        "\n",
        "# Sort by test_id to match sample_submission.csv order\n",
        "submission_df = submission_df.sort_values(\"test_id\").reset_index(drop=True)\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv(\"submission1105_20.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_5JO2GeWbu4"
      },
      "source": [
        "**Submission**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XrrnQ-LyWbu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea405ca5-2ded-4467-ab11-aafd045228a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted class distribution:\n",
            "AnnualCrop: 490\n",
            "Forest: 280\n",
            "HerbaceousVegetation: 338\n",
            "Highway: 392\n",
            "Industrial: 174\n",
            "Pasture: 799\n",
            "PermanentCrop: 464\n",
            "Residential: 737\n",
            "River: 103\n",
            "SeaLake: 455\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Extract just the predicted labels\n",
        "predicted_labels = [label for _, label in results]\n",
        "\n",
        "# Count occurrences of each class\n",
        "class_counts = Counter(predicted_labels)\n",
        "\n",
        "# Get all possible classes (in case some are missing from predictions)\n",
        "all_classes = sorted(idx_to_class.values())\n",
        "\n",
        "print(\"\\nPredicted class distribution:\")\n",
        "for class_name in all_classes:\n",
        "    count = class_counts.get(class_name, 0)\n",
        "    print(f\"{class_name}: {count}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 11233833,
          "sourceId": 94510,
          "sourceType": "competition"
        },
        {
          "datasetId": 7225452,
          "sourceId": 11520825,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}